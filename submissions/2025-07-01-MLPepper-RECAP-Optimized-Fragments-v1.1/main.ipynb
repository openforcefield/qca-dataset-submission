{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a7c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, sys, os\n",
    "import requests\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import periodictable\n",
    "\n",
    "from qcportal.external import scaffold\n",
    "from qcportal import PortalClient\n",
    "from qcportal.serialization import encode_to_json\n",
    "from qcportal.optimization import OptimizationDatasetEntry\n",
    "from qcportal.torsiondrive import TorsiondriveDatasetEntry\n",
    "DatasetEntry = {\"optimization\": OptimizationDatasetEntry, \"torsiondrive\": TorsiondriveDatasetEntry}\n",
    "\n",
    "ADDRESS = \"https://api.qcarchive.molssi.org:443/\"\n",
    "#client = PortalClient(ADDRESS, cache_dir=\".\")\n",
    "client = PortalClient(\n",
    "    ADDRESS, \n",
    "    username=os.environ['QCARCHIVE_USER'],\n",
    "    password=os.environ['QCARCHIVE_PASSWORD'],\n",
    "    cache_dir=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865126a7",
   "metadata": {},
   "source": [
    "# Get Datasets Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting datasets\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Record IDs of Relevant Datasets ____________\n",
    "print(\"Getting datasets\")\n",
    "\n",
    "datasets = [\n",
    "    client.get_dataset(\"singlepoint\", \"MLPepper RECAP Optimized Fragments v1.0\"),\n",
    "    client.get_dataset(\"singlepoint\", \"MLPepper RECAP Optimized Fragments v1.0 Add Iodines\"),\n",
    "]\n",
    "dataset_type = datasets[0].dataset_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a76ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    }
   ],
   "source": [
    "# _________ Get Records ____________\n",
    "print(\"Getting records\")\n",
    "records = []\n",
    "entry_spec_by_ds_id = defaultdict(lambda: defaultdict(list))\n",
    "for ds in datasets:\n",
    "    for entry_name, spec_name, rec in ds.iterate_records():\n",
    "        records.append(rec)\n",
    "        entry_spec_by_ds_id[ds.id][spec_name].append(entry_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2fbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 150194 records and 56351 unique SMILES strings (unique molecules)\n"
     ]
    }
   ],
   "source": [
    "cmiles_count = defaultdict(Counter)\n",
    "molecules = []\n",
    "for rec in records:\n",
    "    cmiles = rec.molecule.extras['canonical_isomeric_explicit_hydrogen_mapped_smiles']\n",
    "\n",
    "    if cmiles not in cmiles_count:\n",
    "        molecules.append(rec.molecule)\n",
    "    hash = rec.molecule.get_hash()\n",
    "    cmiles_count[cmiles][hash] += 1\n",
    "\n",
    "print(f\"There are {len(records)} records and {len(cmiles_count)} unique SMILES strings (unique molecules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32e1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Molecular Statistics\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Statistics from Dataset ____________\n",
    "     \n",
    "print(\"Generating Molecular Statistics\")\n",
    "\n",
    "lx = len(cmiles_count)\n",
    "n_confs, n_heavy_atoms, masses, unique_charges = np.zeros(lx), [], np.zeros(lx), np.zeros(lx)\n",
    "elements = []\n",
    "for i, (cmiles, hashes) in enumerate(cmiles_count.items()):\n",
    "    n_confs[i] = len(hashes)\n",
    "    n_heavy_atoms.append(len([x for x in molecules[i].symbols if x != \"H\"]))\n",
    "    elements.extend(list(set([x for x in molecules[i].symbols])))\n",
    "    masses[i] = sum([getattr(periodictable, x).mass for x in molecules[i].symbols])\n",
    "    unique_charges[i] = molecules[i].molecular_charge\n",
    "    \n",
    "unique_charges = sorted(set(unique_charges))\n",
    "\n",
    "elements = sorted(list(set(elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Heavy Atom Counts\n",
      "  1: 1\n",
      "  2: 2\n",
      "  3: 73\n",
      "  4: 197\n",
      "  5: 558\n",
      "  6: 1338\n",
      "  7: 2957\n",
      "  8: 5728\n",
      "  9: 8617\n",
      " 10: 10872\n",
      " 11: 12697\n",
      " 12: 12632\n",
      " 13: 52\n",
      " 14: 102\n",
      " 15: 61\n",
      " 16: 90\n",
      " 17: 128\n",
      " 18: 111\n",
      " 19: 71\n",
      " 20: 46\n",
      " 21: 2\n",
      " 22: 2\n",
      " 23: 9\n",
      " 24: 2\n",
      " 25: 1\n",
      " 29: 2\n",
      "* Number of unique molecules: 56351\n",
      "* Number of conformers: 75097\n",
      "* Number of conformers (min, mean, max): 1.00, 1.33, 5.00\n",
      "* Molecular weight (min, mean, max): 32.12, 163.20, 701.59\n",
      "* Charges: -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 1 (Run Before Approval) ____________\n",
    "\n",
    "print(\"\\n# Heavy Atom Counts\")\n",
    "counts1 = Counter(n_heavy_atoms)\n",
    "for n_heavy in sorted(counts1):\n",
    "    print(f\"{str(n_heavy):>3}: {counts1[n_heavy]}\")\n",
    "\n",
    "print(\"* Number of unique molecules: {}\".format(len(cmiles_count)))\n",
    "print(\"* Number of conformers:\", int(sum(n_confs)))\n",
    "print(\n",
    "    \"* Number of conformers (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(n_confs), np.mean(n_confs), max(n_confs)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"* Molecular weight (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(masses), np.mean(masses), max(masses)\n",
    "    )\n",
    ")\n",
    "print(\"* Charges: {}\".format(\", \".join([str(x) for x in unique_charges])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471650",
   "metadata": {},
   "source": [
    "# Make New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new dataset\n"
     ]
    }
   ],
   "source": [
    "# _________ Initialize New Dataset ____________\n",
    "print(\"Initializing new dataset\")\n",
    "with open(\"ds_info.json\") as f:\n",
    "    dataset_information = json.load(f)\n",
    "\n",
    "#dataset = client.get_dataset(dataset_type, dataset_information[\"dataset_name\"])\n",
    "dataset = client.add_dataset(\n",
    "    dataset_type,\n",
    "    dataset_information[\"dataset_name\"],\n",
    "    tagline=dataset_information[\"dataset_tagline\"],\n",
    "    description=dataset_information[\"description\"],\n",
    "    provenance={},\n",
    "    default_tag=\"openff\",\n",
    "    owner_user=\"openffbot\",\n",
    "    tags=[\"openff\"],\n",
    "    extras={\n",
    "        \"submitter\": dataset_information[\"metadata.submitter\"],\n",
    "        \"creation_date\": str(datetime.date.today()),\n",
    "        'collection_type': 'OptimizationDataset',\n",
    "        'long_description': dataset_information[\"description\"],\n",
    "        'long_description_url': dataset_information[\"metadata.long_description_url\"],\n",
    "        \"short_description\": dataset_information[\"dataset_tagline\"],\n",
    "        \"dataset_name\": dataset_information[\"dataset_name\"],\n",
    "        \"elements\": elements,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf34309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset.delete_entries(dataset.entry_names)\n",
    "dataset.delete_specification('wb97x-d/def2-tzvpp')\n",
    "dataset.delete_specification('wb97x-d/def2-tzvpp/ddx-water')\n",
    "print(dataset.record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    },
    {
     "ename": "PortalRequestError",
     "evalue": "Request failed: Cannot copy entries from dataset - destination already has entries with the same name (HTTP status 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPortalRequestError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m#dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=[spec_name], existing_ok=True)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_records_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentry_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecification_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspec_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca/lib/python3.11/site-packages/qcportal/dataset_models.py:2080\u001b[39m, in \u001b[36mBaseDataset.copy_records_from\u001b[39m\u001b[34m(self, source_dataset_id, entry_names, specification_names, existing_ok)\u001b[39m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28mself\u001b[39m.assert_online()\n\u001b[32m   2073\u001b[39m body_data = DatasetCopyFromBody(\n\u001b[32m   2074\u001b[39m     source_dataset_id=source_dataset_id,\n\u001b[32m   2075\u001b[39m     entry_names=make_list(entry_names),\n\u001b[32m   2076\u001b[39m     specification_names=make_list(specification_names),\n\u001b[32m   2077\u001b[39m     copy_records=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2078\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2080\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_base_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/copy_from\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[38;5;28mself\u001b[39m.fetch_entry_names()\n\u001b[32m   2082\u001b[39m \u001b[38;5;28mself\u001b[39m.fetch_specifications()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca/lib/python3.11/site-packages/qcportal/client_base.py:524\u001b[39m, in \u001b[36mPortalClientBase.make_request\u001b[39m\u001b[34m(self, method, endpoint, response_model, body_model, url_params_model, body, url_params, upload_files, allow_retries, additional_headers)\u001b[39m\n\u001b[32m    520\u001b[39m     file_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (serialized_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Just to check my logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserialized_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m d = deserialize(r.content, r.headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca/lib/python3.11/site-packages/qcportal/client_base.py:473\u001b[39m, in \u001b[36mPortalClientBase._request\u001b[39m\u001b[34m(self, method, endpoint, body, url_params, file_data, internal_retry, allow_retries, additional_headers)\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# If this error comes from, ie, the web server or something else, then\u001b[39;00m\n\u001b[32m    470\u001b[39m         \u001b[38;5;66;03m# we have to use 'reason'\u001b[39;00m\n\u001b[32m    471\u001b[39m         details = {\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m: r.reason}\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PortalRequestError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetails[\u001b[33m'\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, r.status_code, details)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[31mPortalRequestError\u001b[39m: Request failed: Cannot copy entries from dataset - destination already has entries with the same name (HTTP status 400)"
     ]
    }
   ],
   "source": [
    "# Get ds associated with specifications\n",
    "# _________ Get Records and Find Associated Dataset Name ____________\n",
    "print(\"Getting records\")\n",
    "\n",
    "for ds_id, spec_entries in entry_spec_by_ds_id.items():\n",
    "    spec_names = list(spec_entries.keys())\n",
    "    entry_names = list({name for names in spec_entries.values() for name in names})\n",
    "    dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=spec_names, existing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9191a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_record_ids = [rec.id for _, _, rec in dataset.iterate_records()]\n",
    "set(new_record_ids) == set([rec.id for rec in records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _________ Write Output Part 2 (Run After Approval) ____________\n",
    "\n",
    "elements = set(\n",
    "    sym\n",
    "    for entry in dataset.iterate_entries()\n",
    "    for sym in entry.initial_molecule.symbols\n",
    ")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 2\\n\")\n",
    "print(\"* Description: {}\".format(dataset.description))\n",
    "print(\"* Purpose: {}\".format(dataset.tagline))\n",
    "print(\"* Name: {}\".format(dataset.name))\n",
    "print(\"* Submitter: {}\\n\".format(dataset.extras[\"submitter\"]))\n",
    "\n",
    "print(\"\\n## Metadata\")\n",
    "print(f\"* Elements: {{{', '.join(elements)}}}\")\n",
    "\n",
    "for spec, obj in dataset.specifications.items():\n",
    "    od = obj.dict()['specification']\n",
    "    print(\"* Program:\", od[\"program\"])\n",
    "    od = od[\"qc_specification\"]\n",
    "    print(\"* QC Specifications:\", spec)\n",
    "    for field, value in od.items():\n",
    "        print(f\"  * {field}: {od[field]}\")\n",
    "    print(\"  * SCF Properties:\")\n",
    "    for field in od[\"keywords\"][\"scf_properties\"]:\n",
    "        print(f\"    * {field}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95201f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold.to_json(dataset, compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
