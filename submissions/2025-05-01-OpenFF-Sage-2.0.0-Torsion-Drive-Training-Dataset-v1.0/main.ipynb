{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a7c165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d593fdf2c6754f6d8b56f3e388561be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import periodictable\n",
    "\n",
    "from qcportal import PortalClient\n",
    "from qcportal.external import scaffold\n",
    "from qcportal.optimization import OptimizationDatasetEntry\n",
    "from qcportal.torsiondrive import TorsiondriveDatasetEntry\n",
    "DatasetEntry = {\"optimization\": OptimizationDatasetEntry, \"torsiondrive\": TorsiondriveDatasetEntry}\n",
    "\n",
    "ADDRESS = \"https://api.qcarchive.molssi.org:443/\"\n",
    "client = PortalClient(ADDRESS, cache_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18183319",
   "metadata": {},
   "source": [
    "# Get Records and Molecular Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting record ids\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Record IDs of Relevant Datasets ____________\n",
    "print(\"Getting record ids\")\n",
    "\n",
    "file = requests.get(\n",
    "    \"https://raw.githubusercontent.com/openforcefield/openff-sage/37a36e7eeaf6cdca795847089a288bdff168c08a/data-set-curation/quantum-chemical/data-sets/1-2-0-td-set.json\"\n",
    ")\n",
    "data = json.loads(file.content)\n",
    "provenance = data[\"provenance\"]\n",
    "# list with: {type, record_id, cmiles, inchi_key}\n",
    "entry_dicts = data[\"entries\"][ADDRESS]\n",
    "dataset_type = \"torsiondrive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2115bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    }
   ],
   "source": [
    "# _________ Get Records ____________\n",
    "print(\"Getting records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483d53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 562\n"
     ]
    }
   ],
   "source": [
    "cmiles_by_record_id = {\n",
    "    int(x[\"record_id\"]): {\"cmiles\": x[\"cmiles\"], \"mol\": None} \n",
    "    for x in entry_dicts\n",
    "}\n",
    "for record in records:\n",
    "    cmiles_by_record_id[record.id][\"mol\"] = record.initial_molecules\n",
    "\n",
    "cmiles_count = defaultdict(Counter)\n",
    "molecules = []\n",
    "torsionstats = []\n",
    "for recid, x in cmiles_by_record_id.items():\n",
    "    cmiles = x[\"cmiles\"]\n",
    "\n",
    "    if cmiles not in cmiles_count:\n",
    "        molecules.append(x[\"mol\"][0])\n",
    "\n",
    "    torsionstats.append(len(x[\"mol\"]))\n",
    "    hash = x[\"mol\"][0].get_hash()\n",
    "    cmiles_count[cmiles][hash] += 1\n",
    "\n",
    "print(len(records), len(cmiles_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289a354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Molecular Statistics\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Statistics from Dataset ____________\n",
    "print(\"Generating Molecular Statistics\")\n",
    "\n",
    "lx = len(cmiles_count)\n",
    "n_confs, n_heavy_atoms, masses, unique_charges = np.zeros(lx), [], np.zeros(lx), np.zeros(lx)\n",
    "elements = []\n",
    "for i, (cmiles, hashes) in enumerate(cmiles_count.items()):\n",
    "    n_confs[i] = len(hashes)\n",
    "    n_heavy_atoms.append(len([x for x in molecules[i].symbols if x != \"H\"]))\n",
    "    elements.extend(list(set([x for x in molecules[i].symbols])))\n",
    "    masses[i] = sum([getattr(periodictable, x).mass for x in molecules[i].symbols])\n",
    "    unique_charges[i] = molecules[i].molecular_charge\n",
    "    \n",
    "unique_charges = sorted(set(unique_charges))\n",
    "\n",
    "elements = sorted(list(set(elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2826ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Heavy Atom Counts\n",
      "  3: 1\n",
      "  4: 3\n",
      "  5: 4\n",
      "  6: 13\n",
      "  7: 18\n",
      "  8: 29\n",
      "  9: 23\n",
      " 10: 41\n",
      " 11: 39\n",
      " 12: 30\n",
      " 13: 44\n",
      " 14: 47\n",
      " 15: 50\n",
      " 16: 25\n",
      " 17: 35\n",
      " 18: 16\n",
      " 19: 25\n",
      " 20: 16\n",
      " 21: 13\n",
      " 22: 10\n",
      " 23: 11\n",
      " 24: 14\n",
      " 25: 9\n",
      " 26: 5\n",
      " 27: 11\n",
      " 28: 15\n",
      " 29: 10\n",
      " 30: 4\n",
      " 32: 1\n",
      "\n",
      "\n",
      "# Output for README Part 1\n",
      "\n",
      "* Number of unique molecules: 562\n",
      "* Number of driven torsions: 713\n",
      "* Number of conformers: 563\n",
      "* Number of conformers (min, mean, max): 1, 1, 2\n",
      "* Molecular weight (min, mean, max): 46.07, 224.91, 503.41\n",
      "* Charges: -1.0, 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 1 (Run Before Approval) ____________\n",
    "\n",
    "print(\"\\n# Heavy Atom Counts\")\n",
    "counts1 = Counter(n_heavy_atoms)\n",
    "for n_heavy in sorted(counts1):\n",
    "    print(f\"{str(n_heavy):>3}: {counts1[n_heavy]}\")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 1\\n\")\n",
    "print(\"* Number of unique molecules: {}\".format(len(cmiles_count)))\n",
    "print(\"* Number of driven torsions: {}\".format(len(records)))\n",
    "print(\"* Number of conformers:\", int(sum(n_confs)))\n",
    "print(\n",
    "    \"* Number of conformers (min, mean, max): {:.0f}, {:.0f}, {:.0f}\".format(\n",
    "        min(n_confs), np.mean(n_confs), max(n_confs)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"* Molecular weight (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(masses), np.mean(masses), max(masses)\n",
    "    )\n",
    ")\n",
    "print(\"* Charges: {}\".format(\", \".join([str(x) for x in unique_charges])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0c6d7",
   "metadata": {},
   "source": [
    "# Make New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529e82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new dataset\n"
     ]
    }
   ],
   "source": [
    "# _________ Initialize New Dataset ____________\n",
    "print(\"Initializing new dataset\")\n",
    "with open(\"ds_info.json\") as f:\n",
    "    dataset_information = json.load(f)\n",
    "\n",
    "dataset = client.get_dataset(dataset_type, dataset_information[\"dataset_name\"])\n",
    "\n",
    "#dataset = client.add_dataset(\n",
    "#    dataset_type,\n",
    "#    dataset_information[\"dataset_name\"],\n",
    "#    tagline=dataset_information[\"dataset_tagline\"],\n",
    "#    description=dataset_information[\"description\"],\n",
    "#    provenance=provenance,\n",
    "#    default_tag=\"openff\",\n",
    "#    owner_user=\"openffbot\",\n",
    "#    extras={\n",
    "#        \"submitter\": dataset_information[\"metadata.submitter\"],\n",
    "#        \"creation_data\": str(datetime.date.today()),\n",
    "#        'collection_type': 'OptimizationDataset',\n",
    "#        'long_description_url': dataset_information[\"metadata.long_description_url\"],\n",
    "#        \"short description\": dataset_information[\"dataset_tagline\"],\n",
    "#        \"dataset_name\": dataset_information[\"dataset_name\"],\n",
    "#        \"elements\": elements,\n",
    "#    },\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e29790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n",
      "Copying entries from ds-256 (1 of 17)\n"
     ]
    },
    {
     "ename": "PortalRequestError",
     "evalue": "Request failed: Failed to authenticate user session or JWT: User {'user_id': None, 'username': None} is not authorized to access '{'type': '/api/v1/datasets'}' (HTTP status 401)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortalRequestError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopying entries from ds-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(records_to_copy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spec_name, entry_names \u001b[38;5;129;01min\u001b[39;00m tmp_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_records_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecification_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mspec_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/dataset_models.py:1998\u001b[0m, in \u001b[0;36mBaseDataset.copy_records_from\u001b[0;34m(self, source_dataset_id, entry_names, specification_names)\u001b[0m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_online()\n\u001b[1;32m   1991\u001b[0m body_data \u001b[38;5;241m=\u001b[39m DatasetCopyFromBody(\n\u001b[1;32m   1992\u001b[0m     source_dataset_id\u001b[38;5;241m=\u001b[39msource_dataset_id,\n\u001b[1;32m   1993\u001b[0m     entry_names\u001b[38;5;241m=\u001b[39mmake_list(entry_names),\n\u001b[1;32m   1994\u001b[0m     specification_names\u001b[38;5;241m=\u001b[39mmake_list(specification_names),\n\u001b[1;32m   1995\u001b[0m     copy_records\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1996\u001b[0m )\n\u001b[0;32m-> 1998\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/copy_from\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_entry_names()\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_specifications()\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/client_base.py:524\u001b[0m, in \u001b[0;36mPortalClientBase.make_request\u001b[0;34m(self, method, endpoint, response_model, body_model, url_params_model, body, url_params, upload_files, allow_retries, additional_headers)\u001b[0m\n\u001b[1;32m    520\u001b[0m     file_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (serialized_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Just to check my logic\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialized_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m d \u001b[38;5;241m=\u001b[39m deserialize(r\u001b[38;5;241m.\u001b[39mcontent, r\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/client_base.py:473\u001b[0m, in \u001b[0;36mPortalClientBase._request\u001b[0;34m(self, method, endpoint, body, url_params, file_data, internal_retry, allow_retries, additional_headers)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;66;03m# If this error comes from, ie, the web server or something else, then\u001b[39;00m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;66;03m# we have to use 'reason'\u001b[39;00m\n\u001b[1;32m    471\u001b[0m         details \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: r\u001b[38;5;241m.\u001b[39mreason}\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortalRequestError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetails[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mstatus_code, details)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mPortalRequestError\u001b[0m: Request failed: Failed to authenticate user session or JWT: User {'user_id': None, 'username': None} is not authorized to access '{'type': '/api/v1/datasets'}' (HTTP status 401)"
     ]
    }
   ],
   "source": [
    "# Get ds associated with specifications\n",
    "# _________ Get Records and Find Associated Dataset Name ____________\n",
    "print(\"Getting records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "records_to_copy = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    try:\n",
    "        response = client.query_dataset_records(record_id=[rec.id])\n",
    "        records_to_copy[response[0][\"dataset_id\"]][response[0][\"specification_name\"]].append(response[0][\"entry_name\"])\n",
    "    except Exception:\n",
    "        print(f\"Failed record {rec.id}, {response}\")\n",
    "    \n",
    "for i, (ds_id, tmp_dict) in enumerate(records_to_copy.items()):\n",
    "    print(f\"Copying entries from ds-{ds_id} ({i+1} of {len(records_to_copy)})\")\n",
    "    for spec_name, entry_names in tmp_dict.items():\n",
    "        dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=[spec_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5db57f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PortalRequestError",
     "evalue": "Request failed: Failed to authenticate user session or JWT: User {'user_id': None, 'username': None} is not authorized to access '{'type': '/api/v1/datasets'}' (HTTP status 401)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortalRequestError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/dataset_models.py:415\u001b[0m, in \u001b[0;36mBaseDataset.submit\u001b[0;34m(self, entry_names, specification_names, compute_tag, compute_priority, find_existing, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk_iterable(entry_names, batch_size), total\u001b[38;5;241m=\u001b[39mn_batches, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    407\u001b[0m     body_data \u001b[38;5;241m=\u001b[39m DatasetSubmitBody(\n\u001b[1;32m    408\u001b[0m         entry_names\u001b[38;5;241m=\u001b[39mentry_batch,\n\u001b[1;32m    409\u001b[0m         specification_names\u001b[38;5;241m=\u001b[39m[spec],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m         find_existing\u001b[38;5;241m=\u001b[39mfind_existing,\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 415\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/submit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInsertCountsMetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     n_inserted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mn_inserted\n\u001b[1;32m    423\u001b[0m     n_existing \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mn_existing\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/client_base.py:524\u001b[0m, in \u001b[0;36mPortalClientBase.make_request\u001b[0;34m(self, method, endpoint, response_model, body_model, url_params_model, body, url_params, upload_files, allow_retries, additional_headers)\u001b[0m\n\u001b[1;32m    520\u001b[0m     file_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (serialized_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Just to check my logic\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialized_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m d \u001b[38;5;241m=\u001b[39m deserialize(r\u001b[38;5;241m.\u001b[39mcontent, r\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/bin/QCFractal/qcportal/qcportal/client_base.py:473\u001b[0m, in \u001b[0;36mPortalClientBase._request\u001b[0;34m(self, method, endpoint, body, url_params, file_data, internal_retry, allow_retries, additional_headers)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;66;03m# If this error comes from, ie, the web server or something else, then\u001b[39;00m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;66;03m# we have to use 'reason'\u001b[39;00m\n\u001b[1;32m    471\u001b[0m         details \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: r\u001b[38;5;241m.\u001b[39mreason}\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortalRequestError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetails[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mstatus_code, details)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mPortalRequestError\u001b[0m: Request failed: Failed to authenticate user session or JWT: User {'user_id': None, 'username': None} is not authorized to access '{'type': '/api/v1/datasets'}' (HTTP status 401)"
     ]
    }
   ],
   "source": [
    "dataset.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eecbc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_record_ids = set([rec.id for _, _, rec in dataset.iterate_records()])\n",
    "old_record_ids = set([rec.id for rec in records])\n",
    "set(new_record_ids) == set(old_record_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75fca7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 712 115\n"
     ]
    }
   ],
   "source": [
    "print(len(old_record_ids), len(new_record_ids), len(new_record_ids - old_record_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b1b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 596\n"
     ]
    }
   ],
   "source": [
    "combined_records = client.get_records(record_ids=list(old_record_ids | new_record_ids))\n",
    "\n",
    "entry_organization = defaultdict(dict)\n",
    "for rec in combined_records:\n",
    "    try:\n",
    "        response = client.query_dataset_records(record_id=[rec.id])\n",
    "        entry_organization[response[0][\"entry_name\"]][rec.id] = response\n",
    "    except Exception:\n",
    "        print(f\"Failed record {rec.id}, {response}\")\n",
    "        \n",
    "entries_new_record = {}\n",
    "entries_merged = {}\n",
    "for entry_name, response_dict in entry_organization.items():\n",
    "    if len(response_dict) == 1:\n",
    "        entries_merged[entry_name] = response_dict\n",
    "    else:\n",
    "        entries_new_record[entry_name] = response_dict\n",
    "\n",
    "print(len(entries_new_record), len(entries_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2237af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18045609, 18536962]\n",
      "True False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'values_changed': {\"root['id']\": {'new_value': 18045609,\n",
       "   'old_value': 18536962},\n",
       "  \"root['created_on']\": {'new_value': '2020-03-13T20:20:21.609930+00:00',\n",
       "   'old_value': '2020-03-24T17:41:35.905767+00:00'},\n",
       "  \"root['modified_on']\": {'new_value': '2020-03-13T20:20:21.609927+00:00',\n",
       "   'old_value': '2020-03-24T17:41:35.905765+00:00'}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepdiff import DeepDiff\n",
    "from qcportal.serialization import encode_to_json\n",
    "\n",
    "index = 2\n",
    "key = list(entries_new_record.keys())[index]\n",
    "entry_keys = list(entries_new_record[key])\n",
    "print(entry_keys)\n",
    "print(entry_keys[0] in new_record_ids, entry_keys[1] in new_record_ids,)\n",
    "record1 = client.get_records(record_ids=[entry_keys[0]])[0]\n",
    "record2 = client.get_records(record_ids=[entry_keys[1]])[0]\n",
    "DeepDiff(encode_to_json(record2), encode_to_json(record1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Output for README Part 2\n",
      "\n",
      "* Description: A quantum chemical (QC) dataset curated to train the OpenFF 2.0.0 Sage torsion potentials. This QC dataset with the OpenFF default level of theory, B3LYP-D3BJ/DZVP, consists of one dimensional torsional profiles used to train torsion parameters. This Generation 2 dataset increases chemical diversity when compared to Generation 1, which are of value to our industry partners. Large molecules (>20 heavy atoms) were also included, offering more flexible molecules and a greater degree of conformational variation which provide intramolecular interactions. This is the complete TorsionDrive dataset used for training OpenFF 2.0.0 Sage, consisting of data drawn from the following datasets: 'OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 2 Coverage', 'OpenFF Gen 2 Torsion Set 3 Pfizer Discrepancy', 'OpenFF Gen 2 Torsion Set 4 eMolecules  - Discrepancy', 'OpenFF Gen 2 Torsion Set 5 Bayer' and 'OpenFF Gen 2 Torsion Set 6 supplemental 2'. The `HydrogenBondFilter(method='baker-hubbard')` filter was applied, and the following record IDs were dropped due to issues with ForceBalance: 6098580, 2703504, 2703505, 18045478. Further information can be found in the curation scripts for the linked repositories.\n",
      "* Purpose: B3LYP-D3BJ/DZVP conformers applicable to drug-like molecules for OpenFF 2.0.0 Sage\n",
      "* Name: OpenFF Sage 2.0.0 Torsion Drive Training Dataset v1.0\n",
      "* Submitter: jaclark5\n",
      "\n",
      "## Metadata\n",
      "* Elements: {P, Br, F, H, Cl, S, O, C, N, I}\n",
      "* Program: torsiondrive\n",
      "* Optimization Specification: geometric\n",
      "* QC Specifications: default\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 2 (Run After Approval) ____________\n",
    "\n",
    "elements = set(\n",
    "    sym\n",
    "    for entry in dataset.iterate_entries()\n",
    "    for sym in entry.initial_molecules[0].symbols\n",
    ")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 2\\n\")\n",
    "print(\"* Description: {}\".format(dataset.description))\n",
    "print(\"* Purpose: {}\".format(dataset.tagline))\n",
    "print(\"* Name: {}\".format(dataset.name))\n",
    "print(\"* Submitter: {}\".format(dataset.extras[\"submitter\"]))\n",
    "\n",
    "print(\"\\n## Metadata\")\n",
    "print(f\"* Elements: {{{', '.join(elements)}}}\")\n",
    "\n",
    "for spec, obj in dataset.specifications.items():\n",
    "    od = obj.dict()['specification']\n",
    "    print(\"* Program:\", od[\"program\"])\n",
    "    od = od[\"optimization_specification\"]\n",
    "    print(\"* Optimization Specification:\", od[\"program\"])\n",
    "    od = od[\"qc_specification\"]\n",
    "    print(\"* QC Specifications:\", spec)\n",
    "    for field, value in od.items():\n",
    "        print(f\"  * {field}: {od[field]}\")\n",
    "    print(\"  * SCF Properties:\")\n",
    "    for field in od[\"keywords\"][\"scf_properties\"]:\n",
    "        print(f\"    * {field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold.to_json(dataset, compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
