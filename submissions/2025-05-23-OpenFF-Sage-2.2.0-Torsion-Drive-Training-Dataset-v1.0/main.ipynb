{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a7c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff\n",
    "import periodictable\n",
    "\n",
    "from qcportal import PortalClient\n",
    "from qcportal.serialization import encode_to_json\n",
    "from qcportal.external import scaffold\n",
    "from qcportal.optimization import OptimizationDatasetEntry\n",
    "from qcportal.torsiondrive import TorsiondriveDatasetEntry\n",
    "DatasetEntry = {\"optimization\": OptimizationDatasetEntry, \"torsiondrive\": TorsiondriveDatasetEntry}\n",
    "\n",
    "ADDRESS = \"https://api.qcarchive.molssi.org:443/\"\n",
    "client = PortalClient(ADDRESS, cache_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18183319",
   "metadata": {},
   "source": [
    "# Get Records and Molecular Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting record ids\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Record IDs of Relevant Datasets ____________\n",
    "print(\"Getting record ids\")\n",
    "\n",
    "file = requests.get(\n",
    "    \"https://raw.githubusercontent.com/openforcefield/sage-2.2.0/5a4b058336506865e85fcdc6fd6d10c745a3fa7c/02_curate-data/output/torsion-training-set.json\"\n",
    ")\n",
    "data = json.loads(file.content)\n",
    "provenance = data[\"provenance\"]\n",
    "# list with: {type, record_id, cmiles, inchi_key}\n",
    "entry_dicts = data[\"entries\"][ADDRESS]\n",
    "dataset_type = \"torsiondrive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2115bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    }
   ],
   "source": [
    "# _________ Get Records ____________\n",
    "print(\"Getting records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483d53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1290 records (conformers) and 956 unique SMILES strings (unique molecules)\n"
     ]
    }
   ],
   "source": [
    "cmiles_by_record_id = {\n",
    "    int(x[\"record_id\"]): {\"cmiles\": x[\"cmiles\"], \"mol\": None} \n",
    "    for x in entry_dicts\n",
    "}\n",
    "for record in records:\n",
    "    cmiles_by_record_id[record.id][\"mol\"] = record.initial_molecules\n",
    "\n",
    "cmiles_count = defaultdict(Counter)\n",
    "molecules = []\n",
    "torsionstats = []\n",
    "for recid, x in cmiles_by_record_id.items():\n",
    "    cmiles = x[\"cmiles\"]\n",
    "\n",
    "    if cmiles not in cmiles_count:\n",
    "        molecules.append(x[\"mol\"][0])\n",
    "\n",
    "    torsionstats.append(len(x[\"mol\"]))\n",
    "    hash = x[\"mol\"][0].get_hash()\n",
    "    cmiles_count[cmiles][hash] += 1\n",
    "\n",
    "print(f\"There are {len(records)} records (conformers) and {len(cmiles_count)} unique SMILES strings (unique molecules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289a354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Molecular Statistics\n",
      "['Br', 'C', 'Cl', 'F', 'H', 'N', 'O', 'P', 'S']\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Statistics from Dataset ____________\n",
    "print(\"Generating Molecular Statistics\")\n",
    "\n",
    "lx = len(cmiles_count)\n",
    "n_confs, n_heavy_atoms, masses, unique_charges = np.zeros(lx), [], np.zeros(lx), np.zeros(lx)\n",
    "elements = []\n",
    "for i, (cmiles, hashes) in enumerate(cmiles_count.items()):\n",
    "    n_confs[i] = len(hashes)\n",
    "    n_heavy_atoms.append(len([x for x in molecules[i].symbols if x != \"H\"]))\n",
    "    elements.extend(list(set([x for x in molecules[i].symbols])))\n",
    "    masses[i] = sum([getattr(periodictable, x).mass for x in molecules[i].symbols])\n",
    "    unique_charges[i] = molecules[i].molecular_charge\n",
    "    \n",
    "unique_charges = sorted(set(unique_charges))\n",
    "\n",
    "elements = sorted([str(x) for x in set(elements)])\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2826ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Heavy Atom Counts\n",
      "  2: 1\n",
      "  3: 2\n",
      "  4: 19\n",
      "  5: 25\n",
      "  6: 65\n",
      "  7: 66\n",
      "  8: 97\n",
      "  9: 85\n",
      " 10: 87\n",
      " 11: 70\n",
      " 12: 49\n",
      " 13: 61\n",
      " 14: 61\n",
      " 15: 54\n",
      " 16: 26\n",
      " 17: 34\n",
      " 18: 16\n",
      " 19: 26\n",
      " 20: 16\n",
      " 21: 13\n",
      " 22: 7\n",
      " 23: 11\n",
      " 24: 13\n",
      " 25: 6\n",
      " 26: 5\n",
      " 27: 11\n",
      " 28: 15\n",
      " 29: 10\n",
      " 30: 4\n",
      " 32: 1\n",
      "\n",
      "\n",
      "# Output for README Part 1\n",
      "\n",
      "* Number of unique molecules: 956\n",
      "* Number of driven torsions: 1290\n",
      "* Number of conformers: 982\n",
      "* Number of conformers (min, mean, max): 1, 1, 3\n",
      "* Molecular weight (min, mean, max): 46.07, 185.48, 503.42\n",
      "* Charges: -1.0, 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 1 (Run Before Approval) ____________\n",
    "\n",
    "print(\"\\n# Heavy Atom Counts\")\n",
    "counts1 = Counter(n_heavy_atoms)\n",
    "for n_heavy in sorted(counts1):\n",
    "    print(f\"{str(n_heavy):>3}: {counts1[n_heavy]}\")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 1\\n\")\n",
    "print(\"* Number of unique molecules: {}\".format(len(cmiles_count)))\n",
    "print(\"* Number of driven torsions: {}\".format(len(records)))\n",
    "print(\"* Number of conformers:\", int(sum(n_confs)))\n",
    "print(\n",
    "    \"* Number of conformers (min, mean, max): {:.0f}, {:.0f}, {:.0f}\".format(\n",
    "        min(n_confs), np.mean(n_confs), max(n_confs)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"* Molecular weight (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(masses), np.mean(masses), max(masses)\n",
    "    )\n",
    ")\n",
    "print(\"* Charges: {}\".format(\", \".join([str(x) for x in unique_charges])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f70386",
   "metadata": {},
   "source": [
    "# Validate Inter-database Record Entry Names and Specifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f197b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect our records to come from the following datasets: [256, 257, 258, 259, 265, 266, 48, 36, 242, 243, 70, 317, 314, 308, 282]\n",
      "There are 0 records that aren't in the datasets that we expect.\n"
     ]
    }
   ],
   "source": [
    "# Get Dataset Ids of Interest:\n",
    "dataset_names = [\n",
    "    \"OpenFF Gen 2 Torsion Set 1 Roche 2\",\n",
    "    \"OpenFF Gen 2 Torsion Set 2 Coverage 2\",\n",
    "    \"OpenFF Gen 2 Torsion Set 3 Pfizer Discrepancy 2\",\n",
    "    \"OpenFF Gen 2 Torsion Set 4 eMolecules Discrepancy 2\",\n",
    "    \"OpenFF Gen 2 Torsion Set 5 Bayer 2\",\n",
    "    \"OpenFF Gen 2 Torsion Set 6 Supplemental 2\",\n",
    "    \"SMIRNOFF Coverage Torsion Set 1\",\n",
    "    \"OpenFF Group1 Torsions\",\n",
    "    \"OpenFF Group1 Torsions 2\",\n",
    "    \"OpenFF Group1 Torsions 3\",\n",
    "    \"Pfizer Discrepancy Torsion Dataset 1\",\n",
    "    \"OpenFF Gen3 Torsion Set v1.0\",\n",
    "    \"OpenFF Amide Torsion Set v1.0\",\n",
    "    \"OpenFF WBO Conjugated Series v1.0\",\n",
    "    \"OpenFF DANCE 1 eMolecules t142 v1.0\",\n",
    "]\n",
    "dataset_ids = [client.get_dataset(dataset_type, ds_name).id for ds_name in dataset_names]\n",
    "print(f\"We expect our records to come from the following datasets: {dataset_ids}\")\n",
    "\n",
    "record_ids = set([int(x[\"record_id\"]) for x in entry_dicts])\n",
    "tmp_ds_ids1 = []\n",
    "wrong_ds1 = defaultdict(list)\n",
    "for rec_id in record_ids:\n",
    "    response = client.query_dataset_records(record_id=[rec_id])\n",
    "    ds_name = None\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] in dataset_names:\n",
    "            tmp_ds_ids1.append(resp[\"dataset_name\"])\n",
    "            ds_name = resp[\"dataset_name\"]\n",
    "    if ds_name is None:\n",
    "        wrong_ds1[rec_id] = [resp[\"dataset_name\"] for resp in response]\n",
    "tmp_ds_ids1 = set(tmp_ds_ids1)\n",
    "print(f\"There are {len(wrong_ds1)} records that aren't in the datasets that we expect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1730c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These records have 3 unique specifications\n"
     ]
    }
   ],
   "source": [
    "def remove_dihedrals(dict1):\n",
    "    del dict1['keywords']['dihedrals']\n",
    "    del dict1['keywords']['dihedral_ranges']\n",
    "    del dict1['keywords']['grid_spacing']\n",
    "    del dict1['keywords']['energy_upper_limit']\n",
    "    return dict1\n",
    "\n",
    "def check_diff(dict1, dict2):\n",
    "    tmp = DeepDiff(dict1, dict2)\n",
    "    return len(tmp) > 0\n",
    "\n",
    "# __________ Check that all records share a single specification __________\n",
    "specification_list = []\n",
    "for rec in records:\n",
    "    tmp = remove_dihedrals(encode_to_json(rec.specification))\n",
    "    if all(check_diff(tmp, x) for x in specification_list) or not specification_list:\n",
    "        specification_list.append(tmp)\n",
    "        \n",
    "print(f\"These records have {len(specification_list)} unique specifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c6118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follow datasets (represented by their ids) share a spec: [[256, 257, 258, 259, 265, 266, 48, 242, 243, 70, 282], [36], [317, 314, 308]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "specification_ds_ids = [[] for _ in range(len(specification_list))]\n",
    "for ds_name in dataset_names:\n",
    "    tmp_ds = client.get_dataset(dataset_type, ds_name)\n",
    "    spec = remove_dihedrals(encode_to_json(tmp_ds.specifications[\"default\"].specification))\n",
    "    for i, ref_spec in enumerate(specification_list):\n",
    "        if len(DeepDiff(spec, ref_spec)) == 0:\n",
    "            specification_ds_ids[i].append(tmp_ds.id)\n",
    "            break\n",
    "print(f\"The follow datasets (represented by their ids) share a spec: {specification_ds_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd35c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry names representing different records when in different datasets.\n",
      "2 c1cc[c:1](cc1)[ch2:2][ch2:3][f:4]\n",
      "     18535805 ['OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762632 ['OpenFF Group1 Torsions']\n",
      "3 c1cc[c:4](cc1)[ch2:3][c:2]2[ch:1]cccc2\n",
      "     18045316 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762050 ['OpenFF Group1 Torsions']\n",
      "2 [ch3:4][ch2:3][c:2]1[ch:1]cco1\n",
      "     18535820 ['OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762651 ['OpenFF Group1 Torsions']\n",
      "2 [ch3:4][ch2:3]/[c:2](=[ch:1]\\c)/c\n",
      "     18535821 ['OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762388 ['OpenFF Group1 Torsions']\n",
      "3 c1[ch:1][c:2](c(=o)[nh]c1)[ch:3]2c=ccc=[ch:4]2\n",
      "     18045320 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762103 ['OpenFF Group1 Torsions']\n",
      "3 cc(=o)[nh:4][c:3]1(cc1)[c:2]2[n:1]ccn2c\n",
      "     18045323 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762489 ['OpenFF Group1 Torsions']\n",
      "3 c1c[ch:1][c:2](cc1)[c:3]2(cc2)[nh:4]c3ccccc3\n",
      "     18045325 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762745 ['OpenFF Group1 Torsions']\n",
      "2 c1c[ch:1][c:2](cc1)[c:3]2=ccc[ch2:4]2\n",
      "     18535839 ['OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762687 ['OpenFF Group1 Torsions']\n",
      "3 c[c:4](=o)[nh:3][c:2]1(cc1)[c:1]2nccn2c\n",
      "     18045337 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762490 ['OpenFF Group1 Torsions']\n",
      "3 c1c[ch:1][c:2](c(c1)c2c=cnc=c2)[n+:3](=o)[o-:4]\n",
      "     18045344 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762095 ['OpenFF Group1 Torsions']\n",
      "2 c[s:3](=o)(=[o:4])[c:2]1[ch:1]cccc1\n",
      "     18535888 ['OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     1762425 ['OpenFF Group1 Torsions']\n",
      "5 cc1ccccc1[c:1]2[n-:2][n:3][n:4]n2\n",
      "     18045364 ['OpenFF Gen 2 Torsion Set 1 Roche', 'OpenFF Gen 2 Torsion Set 1 Roche 2']\n",
      "     2703634 ['SMIRNOFF Coverage Torsion Set 1', 'OpenFF Gen 2 Torsion Set 2 Coverage', 'OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "2 [c@:3]([c:2]([f:1])(f)br)([f:4])(cl)br\n",
      "     18536027 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703514 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 c[c:2](=[ch2:1])[c:3]1(cc1)[ch2:4]on\n",
      "     18536050 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703424 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 c1con[c:1]1[c:2]2(cc2)[c:3](=[o:4])o\n",
      "     18536052 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703245 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 [ch:1]1con[c:2]1[c:3]2(cc2)[c:4](=o)o\n",
      "     18536053 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703240 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 cc[c@h]1[ch2:4][c@:3]1([c:2]2[ch:1]cc(nn2)oc)n\n",
      "     18536055 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703494 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 [ch:1]1[ch:2][o:3][n:4]c1c2(cc2)c(=o)o\n",
      "     18536094 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703238 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "3 c[c@@]1(cc(=o)n1)[c:3]2n[ch:1][ch:2][s:4]2\n",
      "     18536101 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703145 ['SMIRNOFF Coverage Torsion Set 1', 'OpenFF Gen 2 Torsion Set 2 Coverage']\n",
      "3 c1c[ch:1][c:2](cc1)[p:3](=o)([cl:4])cl\n",
      "     18045477 ['OpenFF Gen 2 Torsion Set 2 Coverage', 'OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703557 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 c1cc1nc(=o)c[n@:2]2[ch2:1][c@@h]([ch2:4][o:3]2)o\n",
      "     18536107 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703481 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 [h:1][nh:2][n:3]([h:4])cc1cnns1\n",
      "     18536109 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703309 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 c[c@h]([c:1]1[nh:2][nh:3][c:4](=o)n1)nc#n\n",
      "     18536111 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703567 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "3 [h:4][n:3]([s:2](=o)(=o)[c:1]1ccc(cc1)[n+](=o)[o-])cl\n",
      "     18536118 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703532 ['SMIRNOFF Coverage Torsion Set 1', 'OpenFF Gen 2 Torsion Set 2 Coverage']\n",
      "2 c[ch2:1][o:2][p:3](=o)(n)[o:4]cc\n",
      "     18536124 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703394 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "2 c[c:1](=o)[nh:2][p@:3](=[o:4])(oc)sc\n",
      "     18536129 ['OpenFF Gen 2 Torsion Set 2 Coverage 2']\n",
      "     2703639 ['SMIRNOFF Coverage Torsion Set 1']\n",
      "We will rename all of these entry names later to include their respective dataset id of origin.\n"
     ]
    }
   ],
   "source": [
    "# Determine if multiple datasets have entries with the same name\n",
    "entry_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=[rec.id])\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] != \"OpenFF Sage 2.0.0 Torsion Drive Training Dataset v1.0\":\n",
    "            entry_dict[resp[\"entry_name\"]][\"orig records\"].append((rec.id, resp[\"dataset_name\"]))\n",
    "        \n",
    "print(\"Entry names representing different records when in different datasets.\")\n",
    "repeat_entry_names = defaultdict(list)\n",
    "for entry_name, tmp_record_dict in entry_dict.items():\n",
    "    tmp = tmp_record_dict[\"orig records\"]\n",
    "    if len(tmp) > 1: # entry name is in multiple datasets\n",
    "        tmp_dict = defaultdict(list)\n",
    "        for x in tmp:\n",
    "            tmp_dict[x[0]].append(x[1])\n",
    "        if len(tmp_dict) > 1: # entry name is assigned to multiple different records\n",
    "            print(len(tmp), entry_name)\n",
    "            for rec_id, tmp_ds_names in tmp_dict.items():\n",
    "                repeat_entry_names[entry_name].append(rec_id)\n",
    "                print(\"    \", rec_id, tmp_ds_names)\n",
    "        repeat_entry_names[entry_name] = list(set(repeat_entry_names[entry_name]))\n",
    "print(\"We will rename all of these entry names later to include their respective dataset id of origin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726dcaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\n",
      "True 1290\n"
     ]
    }
   ],
   "source": [
    "# ___________ Check that given a dataset id, entry_name, and spec_name, the same record is returned ________________\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "track_records_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=rec.id)\n",
    "    for resp in response:\n",
    "        tmp_ds = client.get_dataset(dataset_type, resp[\"dataset_name\"])\n",
    "        rec2 = tmp_ds.get_record(resp[\"entry_name\"], resp[\"specification_name\"])\n",
    "        track_records_dict[rec.id == rec2.id][rec.id].append([resp[\"dataset_name\"], resp[\"entry_name\"], resp[\"specification_name\"], rec2.id])\n",
    "        \n",
    "print(\"Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\")\n",
    "for key, value in track_records_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0c6d7",
   "metadata": {},
   "source": [
    "# Make New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529e82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new dataset\n"
     ]
    },
    {
     "ename": "PortalRequestError",
     "evalue": "Request failed: Could not find torsiondrive dataset with name 'OpenFF Sage 2.2.0 Torsion Drive Training Dataset v1.0' (HTTP status 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPortalRequestError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mds_info.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     dataset_information = json.load(f)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_information\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m client.delete_dataset(dataset.id, delete_records=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m dataset = client.add_dataset(\n\u001b[32m     10\u001b[39m     dataset_type,\n\u001b[32m     11\u001b[39m     dataset_information[\u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     },\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca-clean/lib/python3.11/site-packages/qcportal/client.py:313\u001b[39m, in \u001b[36mPortalClient.get_dataset\u001b[39m\u001b[34m(self, dataset_type, dataset_name)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_type: \u001b[38;5;28mstr\u001b[39m, dataset_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    312\u001b[39m     body = DatasetQueryModel(dataset_name=dataset_name, dataset_type=dataset_type)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapi/v1/datasets/query\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset_from_dict(ds, \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapi/v1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca-clean/lib/python3.11/site-packages/qcportal/client_base.py:524\u001b[39m, in \u001b[36mPortalClientBase.make_request\u001b[39m\u001b[34m(self, method, endpoint, response_model, body_model, url_params_model, body, url_params, upload_files, allow_retries, additional_headers)\u001b[39m\n\u001b[32m    520\u001b[39m     file_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (serialized_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Just to check my logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserialized_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m d = deserialize(r.content, r.headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/qca-clean/lib/python3.11/site-packages/qcportal/client_base.py:473\u001b[39m, in \u001b[36mPortalClientBase._request\u001b[39m\u001b[34m(self, method, endpoint, body, url_params, file_data, internal_retry, allow_retries, additional_headers)\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# If this error comes from, ie, the web server or something else, then\u001b[39;00m\n\u001b[32m    470\u001b[39m         \u001b[38;5;66;03m# we have to use 'reason'\u001b[39;00m\n\u001b[32m    471\u001b[39m         details = {\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m: r.reason}\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PortalRequestError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetails[\u001b[33m'\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, r.status_code, details)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[31mPortalRequestError\u001b[39m: Request failed: Could not find torsiondrive dataset with name 'OpenFF Sage 2.2.0 Torsion Drive Training Dataset v1.0' (HTTP status 400)"
     ]
    }
   ],
   "source": [
    "# _________ Initialize New Dataset ____________\n",
    "print(\"Initializing new dataset\")\n",
    "with open(\"ds_info.json\") as f:\n",
    "    dataset_information = json.load(f)\n",
    "\n",
    "dataset = client.get_dataset(dataset_type, dataset_information[\"dataset_name\"])\n",
    "client.delete_dataset(dataset.id, delete_records=False)\n",
    "\n",
    "dataset = client.add_dataset(\n",
    "    dataset_type,\n",
    "    dataset_information[\"dataset_name\"],\n",
    "    tagline=dataset_information[\"dataset_tagline\"],\n",
    "    description=dataset_information[\"description\"],\n",
    "    provenance=provenance,\n",
    "    default_tag=\"openff\",\n",
    "    owner_user=\"openffbot\",\n",
    "    extras={\n",
    "        \"submitter\": dataset_information[\"metadata.submitter\"],\n",
    "        \"creation_data\": str(datetime.date.today()),\n",
    "        'collection_type': 'OptimizationDataset',\n",
    "        'long_description_url': dataset_information[\"metadata.long_description_url\"],\n",
    "        \"short description\": dataset_information[\"dataset_tagline\"],\n",
    "        \"dataset_name\": dataset_information[\"dataset_name\"],\n",
    "        \"elements\": elements,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _________ Organize Records by Dataset ____________\n",
    "print(\"Organizing records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "records_to_copy = defaultdict(lambda: defaultdict(list))\n",
    "record_ids_to_copy = defaultdict(lambda: defaultdict(list))\n",
    "dataset_names_from_id = defaultdict(str)\n",
    "for rec in records:\n",
    "    try:\n",
    "        response = client.query_dataset_records(record_id=rec.id)\n",
    "        if len(response) > 1:\n",
    "            ds_id, spec_name, entry_name, ds_name = None, None, None, None\n",
    "            for resp in response:\n",
    "                if resp[\"dataset_id\"] in dataset_ids:\n",
    "                    ds_id = resp[\"dataset_id\"]\n",
    "                    ds_name = resp[\"dataset_name\"]\n",
    "                    spec_name = resp[\"specification_name\"]\n",
    "                    entry_name = resp[\"entry_name\"]\n",
    "        else:\n",
    "            ds_id = response[0][\"dataset_id\"]\n",
    "            ds_name = response[0][\"dataset_name\"]\n",
    "            spec_name = response[0][\"specification_name\"]\n",
    "            entry_name = response[0][\"entry_name\"]\n",
    "\n",
    "        if ds_id is None:\n",
    "            raise ValueError(f\"This record, {rec.id}, is not found in a target dataset.\")\n",
    "        dataset_names_from_id[ds_id] = ds_name\n",
    "        \n",
    "        records_to_copy[ds_id][spec_name].append(entry_name)\n",
    "        record_ids_to_copy[ds_id][entry_name] = rec.id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed record {rec.id}, {response}, {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _________ Copy Records by Dataset ____________\n",
    "print(\"Copying records\")\n",
    "for i, (ds_id, tmp_dict) in enumerate(records_to_copy.items()):\n",
    "    print(f\"Copying entries from ds-{ds_id} ({i+1} of {len(records_to_copy)})\")\n",
    "    new_spec_name = \"default-\" + \"-\".join([[str(y) for y in x] for x in specification_ds_ids if ds_id in x][0])\n",
    "    for spec_name, entry_names in tmp_dict.items():\n",
    "        dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=[spec_name])\n",
    "\n",
    "        # Handle Repeat Spec Names\n",
    "        dataset.rename_specification({spec_name: new_spec_name})\n",
    "\n",
    "        # Handle Repeat Entry Names\n",
    "        tmp_repeat_entry_names = list(set(repeat_entry_names.keys()) & set(entry_names))\n",
    "        for entry_name in tmp_repeat_entry_names.keys():\n",
    "            if record_ids_to_copy[ds_id][entry_name] in repeat_entry_names[entry_name]:\n",
    "                repeat_entry_names[entry_name].remove(record_ids_to_copy[ds_id][entry_name])\n",
    "            else:\n",
    "                tmp_repeat_entry_names.remove(entry_name)\n",
    "        name_map = {entry_name: f\"{entry_name}-{ds_id}\" for entry_name in tmp_repeat_entry_names}\n",
    "        dataset.rename_entries(name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b10ea",
   "metadata": {},
   "source": [
    "# Validate that New Dataset Contains the Expected Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eecbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_record_ids = set([rec.id for _, _, rec in dataset.iterate_records()])\n",
    "old_record_ids = set([rec.id for rec in records])\n",
    "print(f\"New record IDs match old record IDs: {set(new_record_ids) == set(old_record_ids)}\")\n",
    "print(f\"There are {len(new_record_ids - old_record_ids)} record ids in the new dataset that aren't in the target list\")\n",
    "print(f\"There are {len(old_record_ids - new_record_ids)} record ids in the target list that aren't in the new dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e8dc4",
   "metadata": {},
   "source": [
    "# Write Out Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _________ Write Output Part 2 (Run After Approval) ____________\n",
    "\n",
    "elements = set(\n",
    "    sym\n",
    "    for entry in dataset.iterate_entries()\n",
    "    for sym in entry.initial_molecules[0].symbols\n",
    ")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 2\\n\")\n",
    "print(\"* Description: {}\".format(dataset.description))\n",
    "print(\"* Purpose: {}\".format(dataset.tagline))\n",
    "print(\"* Name: {}\".format(dataset.name))\n",
    "print(\"* Submitter: {}\".format(dataset.extras[\"submitter\"]))\n",
    "\n",
    "print(\"\\n## Metadata\")\n",
    "print(f\"* Elements: {{{', '.join(sorted(elements))}}}\")\n",
    "\n",
    "for spec, obj in dataset.specifications.items():\n",
    "    od = obj.dict()['specification']\n",
    "    print(\"* Program:\", od[\"program\"])\n",
    "    od = od[\"optimization_specification\"]\n",
    "    print(\"* Optimization Specification:\", od[\"program\"])\n",
    "    od = od[\"qc_specification\"]\n",
    "    print(\"* QC Specifications:\", spec)\n",
    "    for field, value in od.items():\n",
    "        print(f\"  * {field}: {od[field]}\")\n",
    "    print(\"  * SCF Properties:\")\n",
    "    for field in od[\"keywords\"][\"scf_properties\"]:\n",
    "        print(f\"    * {field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold.to_json(dataset, compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
