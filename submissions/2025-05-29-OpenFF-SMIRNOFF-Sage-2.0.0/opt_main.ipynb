{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff\n",
    "import periodictable\n",
    "\n",
    "from qcportal.external import scaffold\n",
    "from qcportal import PortalClient\n",
    "from qcportal.serialization import encode_to_json\n",
    "from qcportal.optimization import OptimizationDatasetEntry\n",
    "from qcportal.torsiondrive import TorsiondriveDatasetEntry\n",
    "DatasetEntry = {\"optimization\": OptimizationDatasetEntry, \"torsiondrive\": TorsiondriveDatasetEntry}\n",
    "\n",
    "ADDRESS = \"https://api.qcarchive.molssi.org:443/\"\n",
    "client = PortalClient(ADDRESS, cache_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865126a7",
   "metadata": {},
   "source": [
    "# Get Records and Molecular Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting record ids\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Record IDs of Relevant Datasets ____________\n",
    "print(\"Getting record ids\")\n",
    "\n",
    "file = requests.get(\n",
    "    \"https://raw.githubusercontent.com/openforcefield/openff-sage/37a36e7eeaf6cdca795847089a288bdff168c08a/data-set-curation/quantum-chemical/data-sets/1-2-0-opt-set-v3.json\"\n",
    ")\n",
    "data = json.loads(file.content)\n",
    "provenance = data[\"provenance\"]\n",
    "# list with: {type, record_id, cmiles, inchi_key}\n",
    "entry_dicts = data[\"entries\"][ADDRESS]\n",
    "dataset_type = entry_dicts[0][\"type\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a76ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    }
   ],
   "source": [
    "# _________ Get Records ____________\n",
    "print(\"Getting records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2fbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3663 records (conformers) and 1039 unique SMILES strings (unique molecules)\n"
     ]
    }
   ],
   "source": [
    "cmiles_by_record_id = {\n",
    "    int(x[\"record_id\"]): {\"cmiles\": x[\"cmiles\"], \"mol\": None} \n",
    "    for x in entry_dicts\n",
    "}\n",
    "for record in records:\n",
    "    cmiles_by_record_id[record.id][\"mol\"] = record.initial_molecule\n",
    "    \n",
    "cmiles_count = defaultdict(Counter)\n",
    "molecules = []\n",
    "for recid, x in cmiles_by_record_id.items():\n",
    "    cmiles = x[\"cmiles\"]\n",
    "\n",
    "    if cmiles not in cmiles_count:\n",
    "        molecules.append(x[\"mol\"])\n",
    "    hash = x[\"mol\"].get_hash()\n",
    "    cmiles_count[cmiles][hash] += 1\n",
    "\n",
    "print(f\"There are {len(records)} records (conformers) and {len(cmiles_count)} unique SMILES strings (unique molecules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32e1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Molecular Statistics\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Statistics from Dataset ____________\n",
    "     \n",
    "print(\"Generating Molecular Statistics\")\n",
    "\n",
    "lx = len(cmiles_count)\n",
    "n_confs, n_heavy_atoms, masses, unique_charges = np.zeros(lx), [], np.zeros(lx), np.zeros(lx)\n",
    "elements = []\n",
    "for i, (cmiles, hashes) in enumerate(cmiles_count.items()):\n",
    "    n_confs[i] = len(hashes)\n",
    "    n_heavy_atoms.append(len([x for x in molecules[i].symbols if x != \"H\"]))\n",
    "    elements.extend(list(set([x for x in molecules[i].symbols])))\n",
    "    masses[i] = sum([getattr(periodictable, x).mass for x in molecules[i].symbols])\n",
    "    unique_charges[i] = molecules[i].molecular_charge\n",
    "    \n",
    "unique_charges = sorted(set(unique_charges))\n",
    "\n",
    "elements = sorted(list(set(elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Heavy Atom Counts\n",
      "  4: 2\n",
      "  5: 3\n",
      "  6: 4\n",
      "  7: 7\n",
      "  8: 20\n",
      "  9: 19\n",
      " 10: 46\n",
      " 11: 56\n",
      " 12: 81\n",
      " 13: 81\n",
      " 14: 94\n",
      " 15: 94\n",
      " 16: 59\n",
      " 17: 59\n",
      " 18: 47\n",
      " 19: 38\n",
      " 20: 36\n",
      " 21: 45\n",
      " 22: 30\n",
      " 23: 23\n",
      " 24: 21\n",
      " 25: 29\n",
      " 26: 6\n",
      " 27: 11\n",
      " 28: 16\n",
      " 29: 10\n",
      " 30: 8\n",
      " 31: 16\n",
      " 32: 16\n",
      " 33: 11\n",
      " 34: 11\n",
      " 35: 5\n",
      " 36: 15\n",
      " 37: 7\n",
      " 38: 8\n",
      " 39: 5\n",
      "* Number of unique molecules: 1039\n",
      "* Number of conformers: 3663\n",
      "* Number of conformers (min, mean, max): 1.00, 3.53, 10.00\n",
      "* Molecular weight (min, mean, max): 76.05, 261.37, 544.64\n",
      "* Charges: -2.0, -1.0, 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 1 (Run Before Approval) ____________\n",
    "\n",
    "print(\"\\n# Heavy Atom Counts\")\n",
    "counts1 = Counter(n_heavy_atoms)\n",
    "for n_heavy in sorted(counts1):\n",
    "    print(f\"{str(n_heavy):>3}: {counts1[n_heavy]}\")\n",
    "\n",
    "print(\"* Number of unique molecules: {}\".format(len(cmiles_count)))\n",
    "print(\"* Number of conformers:\", int(sum(n_confs)))\n",
    "print(\n",
    "    \"* Number of conformers (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(n_confs), np.mean(n_confs), max(n_confs)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"* Molecular weight (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(masses), np.mean(masses), max(masses)\n",
    "    )\n",
    ")\n",
    "print(\"* Charges: {}\".format(\", \".join([str(x) for x in unique_charges])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44278d83",
   "metadata": {},
   "source": [
    "# Validate Inter-database Record Entry Names and Specifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989eb5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect our records to come from the following datasets: [251, 253, 255, 254, 270]\n",
      "There are 0 records that aren't in the datasets that we expect.\n"
     ]
    }
   ],
   "source": [
    "# Get Dataset Ids of Interest:\n",
    "dataset_names = [\n",
    "    \"OpenFF Gen 2 Opt Set 1 Roche\",\n",
    "    \"OpenFF Gen 2 Opt Set 2 Coverage\",\n",
    "    \"OpenFF Gen 2 Opt Set 3 Pfizer Discrepancy\",\n",
    "    \"OpenFF Gen 2 Opt Set 4 eMolecules Discrepancy\",\n",
    "    \"OpenFF Gen 2 Opt Set 5 Bayer\",\n",
    "]\n",
    "dataset_ids = [client.get_dataset(dataset_type, ds_name).id for ds_name in dataset_names]\n",
    "print(f\"We expect our records to come from the following datasets: {dataset_ids}\")\n",
    "\n",
    "record_ids = set([int(x[\"record_id\"]) for x in entry_dicts])\n",
    "tmp_ds_ids1 = []\n",
    "wrong_ds1 = defaultdict(list)\n",
    "for rec_id in record_ids:\n",
    "    response = client.query_dataset_records(record_id=[rec_id])\n",
    "    ds_name = None\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] in dataset_names:\n",
    "            tmp_ds_ids1.append(resp[\"dataset_name\"])\n",
    "            ds_name = resp[\"dataset_name\"]\n",
    "    if ds_name is None:\n",
    "        wrong_ds1[rec_id] = [resp[\"dataset_name\"] for resp in response]\n",
    "tmp_ds_ids1 = set(tmp_ds_ids1)\n",
    "print(f\"There are {len(wrong_ds1)} records that aren't in the datasets that we expect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c7c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These records have 1 unique specifications\n"
     ]
    }
   ],
   "source": [
    "# __________ Check that all records share a single specification __________\n",
    "specification_list = []\n",
    "for rec in records:\n",
    "    tmp = encode_to_json(rec.specification)\n",
    "    if all(len(DeepDiff(tmp, x)) > 0 for x in specification_list) or not specification_list:\n",
    "        specification_list.append(tmp)\n",
    "        \n",
    "print(f\"These records have {len(specification_list)} unique specifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8884c3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry names representing different records when in different datasets.\n",
      "We will rename all of these entry names later to include their respective dataset id of origin.\n"
     ]
    }
   ],
   "source": [
    "# Determine if multiple datasets have entries with the same name\n",
    "entry_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=[rec.id])\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] != \"OpenFF Sage 2.0.0 Torsion Drive Training Dataset v1.0\":\n",
    "            entry_dict[resp[\"entry_name\"]][\"orig records\"].append((rec.id, resp[\"dataset_name\"]))\n",
    "        \n",
    "print(\"Entry names representing different records when in different datasets.\")\n",
    "repeat_entry_names = defaultdict(list)\n",
    "for entry_name, tmp_record_dict in entry_dict.items():\n",
    "    tmp = tmp_record_dict[\"orig records\"]\n",
    "    if len(tmp) > 1: # entry name is in multiple datasets\n",
    "        tmp_dict = defaultdict(list)\n",
    "        for x in tmp:\n",
    "            tmp_dict[x[0]].append(x[1])\n",
    "        if len(tmp_dict) > 1: # entry name is assigned to multiple different records\n",
    "            print(len(tmp), entry_name)\n",
    "            for rec_id, tmp_ds_names in tmp_dict.items():\n",
    "                repeat_entry_names[entry_name].append(rec_id)\n",
    "                print(\"    \", rec_id, tmp_ds_names)\n",
    "print(\"We will rename all of these entry names later to include their respective dataset id of origin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d565ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\n",
      "True 3663\n"
     ]
    }
   ],
   "source": [
    "# ___________ Check that given a dataset id, entry_name, and spec_name, the same record is returned ________________\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "track_records_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=rec.id)\n",
    "    for resp in response:\n",
    "        tmp_ds = client.get_dataset(dataset_type, resp[\"dataset_name\"])\n",
    "        rec2 = tmp_ds.get_record(resp[\"entry_name\"], resp[\"specification_name\"])\n",
    "        track_records_dict[rec.id == rec2.id][rec.id].append([resp[\"dataset_name\"], resp[\"entry_name\"], resp[\"specification_name\"], rec2.id])\n",
    "\n",
    "print(\"Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\")\n",
    "for key, value in track_records_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471650",
   "metadata": {},
   "source": [
    "# Make New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "529e82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new dataset\n"
     ]
    }
   ],
   "source": [
    "# _________ Initialize New Dataset ____________\n",
    "print(\"Initializing new dataset\")\n",
    "with open(\"opt_ds_info.json\") as f:\n",
    "    dataset_information = json.load(f)\n",
    "\n",
    "dataset = client.add_dataset(\n",
    "    dataset_type,\n",
    "    dataset_information[\"dataset_name\"],\n",
    "    tagline=dataset_information[\"dataset_tagline\"],\n",
    "    description=dataset_information[\"description\"],\n",
    "    provenance=provenance,\n",
    "    default_tag=\"openff\",\n",
    "    owner_user=\"openffbot\",\n",
    "    extras={\n",
    "        \"submitter\": dataset_information[\"metadata.submitter\"],\n",
    "        \"creation_data\": str(datetime.date.today()),\n",
    "        'collection_type': 'OptimizationDataset',\n",
    "        'long_description_url': dataset_information[\"metadata.long_description_url\"],\n",
    "        \"short description\": dataset_information[\"dataset_tagline\"],\n",
    "        \"dataset_name\": dataset_information[\"dataset_name\"],\n",
    "        \"elements\": provenance['applied-filters']['ElementFilter-3']['allowed_elements'],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e29790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n",
      "Copying entries from ds-251 (1 of 6)\n",
      "Copying entries from ds-253 (2 of 6)\n",
      "Copying entries from ds-68 (3 of 6)\n",
      "Copying entries from ds-254 (4 of 6)\n",
      "Copying entries from ds-270 (5 of 6)\n",
      "Copying entries from ds-69 (6 of 6)\n"
     ]
    }
   ],
   "source": [
    "# Get ds associated with specifications\n",
    "# _________ Get Records and Find Associated Dataset Name ____________\n",
    "print(\"Getting records\")\n",
    "\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "records_to_copy = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    try:\n",
    "        response = client.query_dataset_records(record_id=[rec.id])\n",
    "        records_to_copy[response[0][\"dataset_id\"]][response[0][\"specification_name\"]].append(response[0][\"entry_name\"])\n",
    "    except Exception:\n",
    "        print(f\"Failed record {rec.id}, {response}\")\n",
    "    \n",
    "for i, (ds_id, tmp_dict) in enumerate(records_to_copy.items()):\n",
    "    print(f\"Copying entries from ds-{ds_id} ({i+1} of {len(records_to_copy)})\")\n",
    "    for spec_name, entry_names in tmp_dict.items():\n",
    "        dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=[spec_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9191a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_record_ids = [rec.id for _, _, rec in dataset.iterate_records()]\n",
    "set(new_record_ids) == set([rec.id for rec in records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb91182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Output for README Part 2\n",
      "\n",
      "* Description: A quantum chemical (QC) dataset of optimization targets was generated at the OpenFF default level of theory, B3LYP-D3BJ/DZVP, and curated to train the parameters of the [OpenFF 2.0.0 Sage](https://github.com/openforcefield/openff-sage) forcefield. This Generation 2 dataset increases chemical diversity when compared to Generation 1, which are of value to our industry partners. Large molecules (>20 heavy atoms) were also included, including more flexible molecules and a greater degree of conformational variation which provide intramolecular interactions. This is the complete Optimization dataset consisting of molecules from the following datasets: OpenFF Gen 2 Opt Set 1 Roche', 'OpenFF Gen 2 Opt Set 2 Coverage', 'OpenFF Gen 2 Opt Set 3 Pfizer Discrepancy', 'OpenFF Gen 2 Opt Set 4 eMolecules  - Discrepancy', and 'OpenFF Gen 2 Opt Set 5 Bayer'. The filters were applied: following filters were applied: RecordStatusFilter(status=RecordStatusEnum.complete), ConnectivityFilter(tolerance=1.2), UndefinedStereoFilter(), ConformerRMSDFilter(max_conformers=10), and ElementFilter(allowed_elements=['H', 'C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I']). Further information can be found in the curation scripts for the linked repositories.\n",
      "* Purpose: B3LYP-D3BJ/DZVP conformers applicable to drug-like molecules for OpenFF 2.0.0 Sage\n",
      "* Name: OpenFF SMIRNOFF Sage 2.0.0\n",
      "* Submitter: jaclark5\n",
      "\n",
      "\n",
      "## Metadata\n",
      "* Elements: {F, I, N, Cl, H, Br, O, C, P, S}\n",
      "* Program: geometric\n",
      "* QC Specifications: default\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 2 (Run After Approval) ____________\n",
    "\n",
    "elements = set(\n",
    "    sym\n",
    "    for entry in dataset.iterate_entries()\n",
    "    for sym in entry.initial_molecule.symbols\n",
    ")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 2\\n\")\n",
    "print(\"* Description: {}\".format(dataset.description))\n",
    "print(\"* Purpose: {}\".format(dataset.tagline))\n",
    "print(\"* Name: {}\".format(dataset.name))\n",
    "print(\"* Submitter: {}\\n\".format(dataset.extras[\"submitter\"]))\n",
    "\n",
    "print(\"\\n## Metadata\")\n",
    "print(f\"* Elements: {{{', '.join(elements)}}}\")\n",
    "\n",
    "for spec, obj in dataset.specifications.items():\n",
    "    od = obj.dict()['specification']\n",
    "    print(\"* Program:\", od[\"program\"])\n",
    "    od = od[\"qc_specification\"]\n",
    "    print(\"* QC Specifications:\", spec)\n",
    "    for field, value in od.items():\n",
    "        print(f\"  * {field}: {od[field]}\")\n",
    "    print(\"  * SCF Properties:\")\n",
    "    for field in od[\"keywords\"][\"scf_properties\"]:\n",
    "        print(f\"    * {field}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95201f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold.to_json(dataset, filename=\"scaffold_opt.json\", compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qca-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
