{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a7c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff\n",
    "import periodictable\n",
    "\n",
    "import qcportal\n",
    "from qcportal.serialization import encode_to_json\n",
    "from qcportal.external import scaffold\n",
    "from qcportal.optimization import OptimizationDatasetEntry\n",
    "from qcportal.torsiondrive import TorsiondriveDatasetEntry\n",
    "DatasetEntry = {\"optimization\": OptimizationDatasetEntry, \"torsiondrive\": TorsiondriveDatasetEntry}\n",
    "\n",
    "ADDRESS = \"https://api.qcarchive.molssi.org:443/\"\n",
    "client = qcportal.PortalClient(\n",
    "    ADDRESS, \n",
    "    username=os.environ['QCARCHIVE_USER'],\n",
    "    password=os.environ['QCARCHIVE_PASSWORD'],\n",
    "    cache_dir=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865126a7",
   "metadata": {},
   "source": [
    "# Get Records and Molecular Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting record ids\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Record IDs of Relevant Datasets ____________\n",
    "print(\"Getting record ids\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/openforcefield/ash-sage-rc2/32345dddeb6cb249367059fd99607ac2950a5c86/03_fit-valence/02_curate-data/output/optimizations-single-v3.json\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an error if the request fails\n",
    "data = response.json()  # Load the JSON content into a Python dictionary\n",
    "\n",
    "entry_dicts = data[\"entries\"][ADDRESS]\n",
    "types = set(entry[\"type\"] for entry in entry_dicts)\n",
    "record_ids = [entry[\"record_id\"] for entry in entry_dicts]\n",
    "if len(types) != 1:\n",
    "    raise ValueError(f\"Expected exactly one unique type, but found: {types}\")\n",
    "dataset_type = types.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a76ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records\n"
     ]
    }
   ],
   "source": [
    "# _________ Get Records ____________\n",
    "print(\"Getting records\")\n",
    "records = client.get_records(record_ids, missing_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2fbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4696 records (conformers) and 4696 unique SMILES strings (unique molecules)\n"
     ]
    }
   ],
   "source": [
    "cmiles_by_record_id = {\n",
    "    int(x[\"record_id\"]): {\"cmiles\": x[\"cmiles\"], \"mol\": None} \n",
    "    for x in entry_dicts\n",
    "}\n",
    "for record in records:\n",
    "    cmiles_by_record_id[record.id][\"mol\"] = record.initial_molecule\n",
    "    \n",
    "cmiles_count = defaultdict(Counter)\n",
    "molecules = []\n",
    "for recid, x in cmiles_by_record_id.items():\n",
    "    cmiles = x[\"cmiles\"]\n",
    "\n",
    "    if cmiles not in cmiles_count:\n",
    "        molecules.append(x[\"mol\"])\n",
    "    hash = x[\"mol\"].get_hash()\n",
    "    cmiles_count[cmiles][hash] += 1\n",
    "    \n",
    "print(f\"There are {len(records)} records (conformers) and {len(cmiles_count)} unique SMILES strings (unique molecules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32e1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Molecular Statistics\n"
     ]
    }
   ],
   "source": [
    "# _________ Pull Statistics from Dataset ____________\n",
    "     \n",
    "print(\"Generating Molecular Statistics\")\n",
    "\n",
    "lx = len(cmiles_count)\n",
    "n_confs, n_heavy_atoms, masses, unique_charges = np.zeros(lx), [], np.zeros(lx), np.zeros(lx)\n",
    "elements = []\n",
    "for i, (cmiles, hashes) in enumerate(cmiles_count.items()):\n",
    "    n_confs[i] = len(hashes)\n",
    "    n_heavy_atoms.append(len([x for x in molecules[i].symbols if x != \"H\"]))\n",
    "    elements.extend(list(set([x for x in molecules[i].symbols])))\n",
    "    masses[i] = sum([getattr(periodictable, x).mass for x in molecules[i].symbols])\n",
    "    unique_charges[i] = molecules[i].molecular_charge\n",
    "    \n",
    "unique_charges = sorted(set(unique_charges))\n",
    "\n",
    "elements = sorted(list(set(elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Heavy Atom Counts\n",
      "  2: 5\n",
      "  3: 10\n",
      "  4: 53\n",
      "  5: 123\n",
      "  6: 225\n",
      "  7: 308\n",
      "  8: 352\n",
      "  9: 365\n",
      " 10: 491\n",
      " 11: 383\n",
      " 12: 294\n",
      " 13: 253\n",
      " 14: 271\n",
      " 15: 255\n",
      " 16: 197\n",
      " 17: 178\n",
      " 18: 134\n",
      " 19: 113\n",
      " 20: 72\n",
      " 21: 82\n",
      " 22: 76\n",
      " 23: 73\n",
      " 24: 49\n",
      " 25: 43\n",
      " 26: 44\n",
      " 27: 24\n",
      " 28: 14\n",
      " 29: 21\n",
      " 30: 16\n",
      " 31: 22\n",
      " 32: 18\n",
      " 33: 11\n",
      " 34: 14\n",
      " 35: 12\n",
      " 36: 13\n",
      " 37: 13\n",
      " 38: 16\n",
      " 39: 6\n",
      " 41: 3\n",
      " 42: 5\n",
      " 43: 5\n",
      " 44: 5\n",
      " 45: 2\n",
      " 46: 3\n",
      " 47: 6\n",
      " 48: 3\n",
      " 49: 2\n",
      " 50: 1\n",
      " 52: 2\n",
      " 53: 3\n",
      " 54: 1\n",
      " 55: 1\n",
      " 57: 1\n",
      " 58: 2\n",
      " 61: 2\n",
      "* Number of unique molecules: 4696\n",
      "* Number of conformers: 4696\n",
      "* Number of conformers (min, mean, max): 1.00, 1.00, 1.00\n",
      "* Molecular weight (min, mean, max): 32.05, 207.67, 878.25\n",
      "* Charges: -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 1 (Run Before Approval) ____________\n",
    "\n",
    "print(\"\\n# Heavy Atom Counts\")\n",
    "counts1 = Counter(n_heavy_atoms)\n",
    "for n_heavy in sorted(counts1):\n",
    "    print(f\"{str(n_heavy):>3}: {counts1[n_heavy]}\")\n",
    "\n",
    "print(\"* Number of unique molecules: {}\".format(len(cmiles_count)))\n",
    "print(\"* Number of conformers:\", int(sum(n_confs)))\n",
    "print(\n",
    "    \"* Number of conformers (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(n_confs), np.mean(n_confs), max(n_confs)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"* Molecular weight (min, mean, max): {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "        min(masses), np.mean(masses), max(masses)\n",
    "    )\n",
    ")\n",
    "print(\"* Charges: {}\".format(\", \".join([str(x) for x in unique_charges])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490f387",
   "metadata": {},
   "source": [
    "# Validate Inter-database Record Entry Names and Specifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053c54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect our records to come from the following datasets: [41, 43, 45, 50, 281, 68, 69, 232, 251, 253, 255, 254, 270, 296, 315, 453, 345, 365, 372, 379, 383, 385, 387, 388, 392, 393, 396, 399, 412, 415, 416, 426]\n",
      "There are 0 records that aren't in the datasets that we expect.\n",
      "There are 0 records that aren't in the datasets that we expect.\n"
     ]
    }
   ],
   "source": [
    "# Get Dataset Ids of Interest:\n",
    "dataset_names = [\n",
    "    \"OpenFF Optimization Set 1\",\n",
    "    \"SMIRNOFF Coverage Set 1\",\n",
    "    \"OpenFF VEHICLe Set 1\",\n",
    "    \"OpenFF Discrepancy Benchmark 1\",\n",
    "    \"OpenFF Ehrman Informative Optimization v0.2\",\n",
    "    \"Pfizer Discrepancy Optimization Dataset 1\",\n",
    "    \"FDA Optimization Dataset 1\",\n",
    "    \"Kinase Inhibitors: WBO Distributions\",\n",
    "    \"OpenFF Gen 2 Opt Set 1 Roche\",\n",
    "    \"OpenFF Gen 2 Opt Set 2 Coverage\",\n",
    "    \"OpenFF Gen 2 Opt Set 3 Pfizer Discrepancy\",\n",
    "    \"OpenFF Gen 2 Opt Set 4 eMolecules Discrepancy\",\n",
    "    \"OpenFF Gen 2 Opt Set 5 Bayer\",\n",
    "    \"OpenFF Sandbox CHO PhAlkEthOH v1.0\",\n",
    "    \"OpenFF Aniline Para Opt v1.0\",\n",
    "    \"OpenFF Industry Benchmark Season 1 v1.2\",\n",
    "    \"OpenFF Gen2 Optimization Dataset Protomers v1.0\",\n",
    "    \"OpenFF Protein Capped 1-mers 3-mers Optimization Dataset v1.0\",\n",
    "    \"OpenFF Iodine Chemistry Optimization Dataset v1.0\",\n",
    "    \"XtalPi Shared Fragments OptimizationDataset v1.0\",\n",
    "    \"XtalPi 20-percent Fragments OptimizationDataset v1.0\",\n",
    "    \"OpenFF Torsion Benchmark Supplement v1.0\",\n",
    "    \"OpenFF Torsion Multiplicity Optimization Training Coverage Supplement v1.0\",\n",
    "    \"OpenFF Torsion Multiplicity Optimization Benchmarking Coverage Supplement v1.0\",\n",
    "    \"OpenFF Iodine Fragment Opt v1.0\",\n",
    "    \"OpenFF Sulfur Optimization Training Coverage Supplement v1.0\",\n",
    "    \"OpenFF Sulfur Optimization Benchmarking Coverage Supplement v1.0\",\n",
    "    \"OpenFF Lipid Optimization Training Supplement v1.0\",\n",
    "    \"OpenFF Lipid Optimization Benchmark Supplement v1.0\",\n",
    "    \"OpenFF Cresset Additional Coverage Optimizations v4.0\",\n",
    "    \"OpenFF Protein PDB 4-mers v4.0\",\n",
    "    \"OpenFF Additional Generated ChEMBL Optimizations v4.0\"\n",
    "]\n",
    "dataset_ids = [client.get_dataset(dataset_type, ds_name).id for ds_name in dataset_names]\n",
    "print(f\"We expect our records to come from the following datasets: {dataset_ids}\")\n",
    "\n",
    "record_ids = set([int(x[\"record_id\"]) for x in entry_dicts])\n",
    "tmp_ds_ids1 = []\n",
    "wrong_ds1 = defaultdict(list)\n",
    "for rec_id in record_ids:\n",
    "    response = client.query_dataset_records(record_id=[rec_id])\n",
    "    ds_name = None\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] in dataset_names:\n",
    "            tmp_ds_ids1.append(resp[\"dataset_name\"])\n",
    "            ds_name = resp[\"dataset_name\"]\n",
    "    if ds_name is None:\n",
    "        wrong_ds1[rec_id] = [resp[\"dataset_name\"] for resp in response]\n",
    "tmp_ds_ids1 = set(tmp_ds_ids1)\n",
    "print(f\"There are {len(wrong_ds1)} records that aren't in the datasets that we expect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e4c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These records have 9 unique specifications\n"
     ]
    }
   ],
   "source": [
    "# __________ Check that all records share a single specification __________\n",
    "specification_list = []\n",
    "for rec in records:\n",
    "    tmp = encode_to_json(rec.specification)\n",
    "    if \"constraints\" in tmp[\"keywords\"]:\n",
    "        del tmp['keywords']['constraints']\n",
    "    if all(len(DeepDiff(tmp, x)) > 0 for x in specification_list) or not specification_list:\n",
    "        specification_list.append(tmp)\n",
    "        \n",
    "print(f\"These records have {len(specification_list)} unique specifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503e3b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follow datasets (represented by their ids) share a spec: [[379, 383, 385, 387, 388, 392, 393, 396, 399, 412, 415, 416, 426], [281], [315, 453, 372], [43, 45, 50], [68, 69, 251, 253, 255, 254, 270], [345, 365], [296], [41], [232]]\n"
     ]
    }
   ],
   "source": [
    "specification_ds_ids = [[] for _ in range(len(specification_list))]\n",
    "for ds_name in dataset_names:\n",
    "    tmp_ds = client.get_dataset(dataset_type, ds_name)\n",
    "    spec = encode_to_json(tmp_ds.specifications[\"default\"].specification)\n",
    "    if \"keywords\" in spec and \"constraints\" in spec[\"keywords\"]:\n",
    "        del spec['keywords']['constraints']\n",
    "    for i, ref_spec in enumerate(specification_list):\n",
    "        if len(DeepDiff(spec, ref_spec)) == 0:\n",
    "            specification_ds_ids[i].append(tmp_ds.id)\n",
    "            break\n",
    "print(f\"The follow datasets (represented by their ids) share a spec: {specification_ds_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7355227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection error for https://api.qcarchive.molssi.org:443/api/v1/datasets/queryrecords: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) - retrying in 0.50 seconds [1/5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry names representing different records when in different datasets.\n",
      "2 C[N+](C)(C)c1ccccc1\n",
      "     127904750 ['OpenFF Torsion Benchmark Supplement v1.0']\n",
      "     137157785 ['OpenFF Torsion Multiplicity Optimization Benchmarking Coverage Supplement v1.0']\n",
      "2 C1CSSC1\n",
      "     120134272 ['XtalPi 20-percent Fragments OptimizationDataset v1.0']\n",
      "     138533300 ['OpenFF Lipid Optimization Training Supplement v1.0']\n",
      "2 C1C[N+]23CCC[N+]2(C1)CCC3\n",
      "     127904781 ['OpenFF Torsion Benchmark Supplement v1.0']\n",
      "     137149064 ['OpenFF Torsion Multiplicity Optimization Training Coverage Supplement v1.0']\n",
      "2 C[N+](C)(C)C1(CC1)C(=O)[O-]\n",
      "     127904748 ['OpenFF Torsion Benchmark Supplement v1.0']\n",
      "     137157784 ['OpenFF Torsion Multiplicity Optimization Benchmarking Coverage Supplement v1.0']\n",
      "2 C(CBr)Br\n",
      "     127904742 ['OpenFF Torsion Benchmark Supplement v1.0']\n",
      "     146497251 ['OpenFF Additional Generated ChEMBL Optimizations v4.0']\n",
      "We will rename all of these entry names later to include their respective dataset id of origin.\n"
     ]
    }
   ],
   "source": [
    "# Determine if multiple datasets have entries with the same name\n",
    "entry_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=[rec.id])\n",
    "    for resp in response:\n",
    "        if resp[\"dataset_name\"] != \"OpenFF Sage 2.0.0 Torsion Drive Training Dataset v1.0\":\n",
    "            entry_dict[resp[\"entry_name\"]][\"orig records\"].append((rec.id, resp[\"dataset_name\"]))\n",
    "        \n",
    "print(\"Entry names representing different records when in different datasets.\")\n",
    "repeat_entry_names = defaultdict(list)\n",
    "for entry_name, tmp_record_dict in entry_dict.items():\n",
    "    tmp = tmp_record_dict[\"orig records\"]\n",
    "    if len(tmp) > 1: # entry name is in multiple datasets\n",
    "        tmp_dict = defaultdict(list)\n",
    "        for x in tmp:\n",
    "            tmp_dict[x[0]].append(x[1])\n",
    "        if len(tmp_dict) > 1: # entry name is assigned to multiple different records\n",
    "            print(len(tmp), entry_name)\n",
    "            for rec_id, tmp_ds_names in tmp_dict.items():\n",
    "                repeat_entry_names[entry_name].append(rec_id)\n",
    "                print(\"    \", rec_id, tmp_ds_names)\n",
    "print(\"We will rename all of these entry names later to include their respective dataset id of origin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4d71ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\n",
      "True 4696\n"
     ]
    }
   ],
   "source": [
    "# ___________ Check that given a dataset id, entry_name, and spec_name, the same record is returned ________________\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "track_records_dict = defaultdict(lambda: defaultdict(list))\n",
    "for rec in records:\n",
    "    response = client.query_dataset_records(record_id=rec.id)\n",
    "    for resp in response:\n",
    "        tmp_ds = client.get_dataset(dataset_type, resp[\"dataset_name\"])\n",
    "        rec2 = tmp_ds.get_record(resp[\"entry_name\"], resp[\"specification_name\"])\n",
    "        track_records_dict[rec.id == rec2.id][rec.id].append([resp[\"dataset_name\"], resp[\"entry_name\"], resp[\"specification_name\"], rec2.id])\n",
    "\n",
    "print(\"Given a record id, ensure that the same record is returned given a dataset name, entry name, and spec name\")\n",
    "for key, value in track_records_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471650",
   "metadata": {},
   "source": [
    "# Make New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529e82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _________ Initialize New Dataset ____________\n",
    "print(\"Initializing new dataset\")\n",
    "\n",
    "with open(\"opt_ds_info.json\") as f:\n",
    "    dataset_information = json.load(f)\n",
    "\n",
    "#dataset = client.get_dataset_by_id(447)\n",
    "dataset = client.add_dataset(\n",
    "    dataset_type,\n",
    "    dataset_information[\"dataset_name\"],\n",
    "    tagline=dataset_information[\"dataset_tagline\"],\n",
    "    description=dataset_information[\"description\"],\n",
    "    tags=[\"openff\"],\n",
    "    provenance={\n",
    "        \"qcportal\": qcportal.__version__,\n",
    "    },\n",
    "    extras={\n",
    "        \"submitter\": dataset_information[\"metadata.submitter\"],\n",
    "        \"creation_date\": str(datetime.date.today()),\n",
    "        'collection_type': 'OptimizationDataset',\n",
    "        \"long_description\": dataset_information[\"description\"],\n",
    "        'long_description_url': dataset_information[\"metadata.long_description_url\"],\n",
    "        \"short description\": dataset_information[\"dataset_tagline\"],\n",
    "        \"dataset_name\": dataset_information[\"dataset_name\"],\n",
    "        \"elements\": elements,\n",
    "    },\n",
    ")\n",
    "dataset.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcda5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 records, 0 specifications, and 0 entries\n"
     ]
    }
   ],
   "source": [
    "dataset.delete_entries(dataset.entry_names)\n",
    "for spec_name in dataset.specifications.keys():\n",
    "    dataset.delete_specification(spec_name)\n",
    "    \n",
    "print(f\"There are {dataset.record_count} records, {len(dataset.specifications)} specifications, and {len(dataset.entry_names)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fe73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing records\n"
     ]
    }
   ],
   "source": [
    "# _________ Organize Records by Dataset ____________\n",
    "spec_index = {ds_id: next(i for i, sublist in enumerate(specification_ds_ids) if ds_id in sublist) for ds_id in dataset_ids}\n",
    "print(\"Organizing records\")\n",
    "records = client.get_records([int(x[\"record_id\"]) for x in entry_dicts], missing_ok=False)\n",
    "records_to_copy = [defaultdict(list) for _ in range(len(specification_ds_ids))]\n",
    "record_ids_to_copy = defaultdict(lambda: defaultdict(list))\n",
    "record_dataset = defaultdict(int)\n",
    "dataset_names_from_id = defaultdict(str)\n",
    "for rec in records:\n",
    "    try:\n",
    "        response = client.query_dataset_records(record_id=rec.id)\n",
    "        if len(response) > 1:\n",
    "            ds_id, spec_name, entry_name, ds_name = None, None, None, None\n",
    "            for resp in response:\n",
    "                if resp[\"dataset_id\"] in dataset_ids:\n",
    "                    ds_id = resp[\"dataset_id\"]\n",
    "                    ds_name = resp[\"dataset_name\"]\n",
    "                    spec_name = resp[\"specification_name\"]\n",
    "                    entry_name = resp[\"entry_name\"]\n",
    "        else:\n",
    "            ds_id = response[0][\"dataset_id\"]\n",
    "            ds_name = response[0][\"dataset_name\"]\n",
    "            spec_name = response[0][\"specification_name\"]\n",
    "            entry_name = response[0][\"entry_name\"]\n",
    "\n",
    "        if ds_id is None:\n",
    "            raise ValueError(f\"This record, {rec.id}, is not found in a target dataset.\")\n",
    "        dataset_names_from_id[ds_id] = ds_name\n",
    "        \n",
    "        records_to_copy[spec_index[ds_id]][ds_id].append(entry_name)\n",
    "        record_ids_to_copy[ds_id][entry_name] = rec.id\n",
    "        record_dataset[rec.id] = ds_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed record {rec.id}, {response}, {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c410a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4696 entry names, expect 4696\n"
     ]
    }
   ],
   "source": [
    "tmp = sum(len(entry_names) for ds_dict in records_to_copy for _, entry_names in ds_dict.items())\n",
    "print(f\"There are {tmp} entry names, expect {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e29790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying records\n",
      "Copying entries from ds-383 (1 of 13)\n",
      "Copying entries from ds-426 (2 of 13)\n",
      "Copying entries from ds-426 (2 of 13)\n",
      "Copying entries from ds-385 (3 of 13)\n",
      "Copying entries from ds-385 (3 of 13)\n",
      "Copying entries from ds-399 (4 of 13)\n",
      "Copying entries from ds-399 (4 of 13)\n",
      "Copying entries from ds-387 (5 of 13)\n",
      "Copying entries from ds-387 (5 of 13)\n",
      "Copying entries from ds-392 (6 of 13)\n",
      "Copying entries from ds-392 (6 of 13)\n",
      "Copying entries from ds-412 (7 of 13)\n",
      "Copying entries from ds-412 (7 of 13)\n",
      "Copying entries from ds-393 (8 of 13)\n",
      "Copying entries from ds-393 (8 of 13)\n",
      "Copying entries from ds-415 (9 of 13)\n",
      "Copying entries from ds-415 (9 of 13)\n",
      "Copying entries from ds-379 (10 of 13)\n",
      "Copying entries from ds-379 (10 of 13)\n",
      "Copying entries from ds-416 (11 of 13)\n",
      "Copying entries from ds-416 (11 of 13)\n",
      "Copying entries from ds-388 (12 of 13)\n",
      "Copying entries from ds-388 (12 of 13)\n",
      "Copying entries from ds-396 (13 of 13)\n",
      "Copying entries from ds-396 (13 of 13)\n",
      "Copying entries from ds-281 (1 of 1)\n",
      "Copying entries from ds-281 (1 of 1)\n",
      "Copying entries from ds-453 (1 of 3)\n",
      "Copying entries from ds-453 (1 of 3)\n",
      "Copying entries from ds-372 (2 of 3)\n",
      "Copying entries from ds-372 (2 of 3)\n",
      "Copying entries from ds-315 (3 of 3)\n",
      "Copying entries from ds-315 (3 of 3)\n",
      "Copying entries from ds-43 (1 of 3)\n",
      "Copying entries from ds-43 (1 of 3)\n",
      "Copying entries from ds-50 (2 of 3)\n",
      "Copying entries from ds-50 (2 of 3)\n",
      "Copying entries from ds-45 (3 of 3)\n",
      "Copying entries from ds-45 (3 of 3)\n",
      "Copying entries from ds-69 (1 of 7)\n",
      "Copying entries from ds-69 (1 of 7)\n",
      "Copying entries from ds-270 (2 of 7)\n",
      "Copying entries from ds-270 (2 of 7)\n",
      "Copying entries from ds-253 (3 of 7)\n",
      "Copying entries from ds-253 (3 of 7)\n",
      "Copying entries from ds-254 (4 of 7)\n",
      "Copying entries from ds-254 (4 of 7)\n",
      "Copying entries from ds-255 (5 of 7)\n",
      "Copying entries from ds-255 (5 of 7)\n",
      "Copying entries from ds-68 (6 of 7)\n",
      "Copying entries from ds-68 (6 of 7)\n",
      "Copying entries from ds-251 (7 of 7)\n",
      "Copying entries from ds-251 (7 of 7)\n",
      "Copying entries from ds-345 (1 of 2)\n",
      "Copying entries from ds-345 (1 of 2)\n",
      "Copying entries from ds-365 (2 of 2)\n",
      "Copying entries from ds-365 (2 of 2)\n",
      "Copying entries from ds-296 (1 of 1)\n",
      "Copying entries from ds-296 (1 of 1)\n",
      "Copying entries from ds-41 (1 of 1)\n",
      "Copying entries from ds-41 (1 of 1)\n",
      "Copying entries from ds-232 (1 of 1)\n",
      "Copying entries from ds-232 (1 of 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# _________ Copy Records by Dataset ____________\n",
    "print(\"Copying records\")\n",
    "for i, ds_dicts in enumerate(records_to_copy):\n",
    "    old_spec_name = \"default\" \n",
    "    new_spec_name = \"default-\" + \"-\".join([str(x) for x in specification_ds_ids[i]])\n",
    "    \n",
    "    for j, (ds_id, entry_names) in enumerate(ds_dicts.items()):\n",
    "        print(f\"Copying entries from ds-{ds_id} ({j+1} of {len(ds_dicts)})\")\n",
    "\n",
    "        dataset.copy_records_from( ds_id, entry_names=entry_names, specification_names=[old_spec_name])\n",
    "        dataset.fetch_specifications(force_refetch=True)\n",
    "        \n",
    "        # Handle Repeat Entry Names\n",
    "        tmp_repeat_entry_names = list(set(repeat_entry_names.keys()) & set(entry_names))\n",
    "        name_map = {entry_name: f\"{entry_name}-{ds_id}\" for entry_name in tmp_repeat_entry_names}\n",
    "        dataset.rename_entries(name_map)\n",
    "        \n",
    "    # Handle Repeat Spec Names\n",
    "    dataset.rename_specification(old_spec_name, new_spec_name)\n",
    "    dataset.fetch_specifications(force_refetch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347dab1",
   "metadata": {},
   "source": [
    "# Validate that New Dataset Contains the Expected Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9191a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record IDs match old record IDs: True\n",
      "There are 0 record ids in the new dataset that aren't in the target list\n",
      "There are 0 record ids in the target list that aren't in the new dataset\n"
     ]
    }
   ],
   "source": [
    "new_record_ids = set([rec.id for _, _, rec in dataset.iterate_records()])\n",
    "old_record_ids = set([rec.id for rec in records])\n",
    "print(f\"New record IDs match old record IDs: {set(new_record_ids) == set(old_record_ids)}\")\n",
    "print(f\"There are {len(new_record_ids - old_record_ids)} record ids in the new dataset that aren't in the target list\")\n",
    "print(f\"There are {len(old_record_ids - new_record_ids)} record ids in the target list that aren't in the new dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6358a57",
   "metadata": {},
   "source": [
    "# Write Out Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb91182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Output for README Part 2\n",
      "\n",
      "* Description: A quantum chemical (QC) optimization and torsiondrive datasets generated at the OpenFF default level of theory, B3LYP-D3BJ/DZVP, and curated to train parameters in [OpenFF 2.3.0 Sage](https://github.com/openforcefield/ash-sage-rc2) with NAGL partial charge model AshGC. Targets were curated from the following datasets:  OpenFF Optimization Set 1, SMIRNOFF Coverage Set 1, OpenFF VEHICLe Set 1, OpenFF Discrepancy Benchmark 1, OpenFF Ehrman Informative Optimization v0.2, Pfizer discrepancy optimization dataset 1, FDA optimization dataset 1, Kinase Inhibitors: WBO Distributions, OpenFF Gen 2 Opt Set 1 Roche, OpenFF Gen 2 Opt Set 2 Coverage, OpenFF Gen 2 Opt Set 3 Pfizer Discrepancy, OpenFF Gen 2 Opt Set 4 eMolecules Discrepancy, OpenFF Gen 2 Opt Set 5 Bayer, OpenFF Sandbox CHO PhAlkEthOH v1.0, OpenFF Aniline Para Opt v1.0, OpenFF Industry Benchmark Season 1 v1.2, OpenFF Gen2 Optimization Dataset Protomers v1.0, OpenFF Protein Capped 1-mers 3-mers Optimization Dataset v1.0, OpenFF Iodine Chemistry Optimization Dataset v1.0, XtalPi Shared Fragments OptimizationDataset v1.0, XtalPi 20-percent Fragments OptimizationDataset v1.0, OpenFF Torsion Benchmark Supplement v1.0, OpenFF Torsion Multiplicity Optimization Training Coverage Supplement v1.0, OpenFF Torsion Multiplicity Optimization Benchmarking Coverage Supplement v1.0, OpenFF Iodine Fragment Opt v1.0, OpenFF Sulfur Optimization Training Coverage Supplement v1.0, OpenFF Sulfur Optimization Benchmarking Coverage Supplement v1.0, OpenFF Lipid Optimization Training Supplement v1.0, OpenFF Lipid Optimization Benchmark Supplement v1.0, OpenFF Cresset Additional Coverage Optimizations v4.0, OpenFF Protein PDB 4-mers v4.0, OpenFF Additional Generated ChEMBL Optimizations v4.0\n",
      "* Purpose: B3LYP-D3BJ/DZVP conformers for training OpenFF 2.2.0 Sage with AshGC NAGL partial charge model.\n",
      "* Name: OpenFF SMIRNOFF Sage 2.3.0\n",
      "* Submitter: jaclark5\n",
      "\n",
      "\n",
      "## Metadata\n",
      "* Elements: {N, P, Cl, O, Br, S, I, H, F, C}\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-232\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3(bj)\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-281\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-296\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-315-453-372\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-345-365\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices', 'mbis_charges']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n",
      "    * mbis_charges\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-379-383-385-387-388-392-393-396-399-412-415-416-426\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-41\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3(bj)\n",
      "  * basis: dzvp\n",
      "  * keywords: {}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-43-45-50\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "\n",
      "* Program: geometric\n",
      "* QC Specifications: default-68-69-251-253-255-254-270\n",
      "  * program: psi4\n",
      "  * driver: SinglepointDriver.deferred\n",
      "  * method: b3lyp-d3bj\n",
      "  * basis: dzvp\n",
      "  * keywords: {'maxiter': 200, 'scf_properties': ['dipole', 'quadrupole', 'wiberg_lowdin_indices', 'mayer_indices']}\n",
      "  * protocols: {'wavefunction': <WavefunctionProtocolEnum.none: 'none'>, 'stdout': True, 'error_correction': {'default_policy': True, 'policies': None}, 'native_files': <NativeFilesProtocolEnum.none: 'none'>}\n",
      "  * SCF Properties:\n",
      "    * dipole\n",
      "    * quadrupole\n",
      "    * wiberg_lowdin_indices\n",
      "    * mayer_indices\n"
     ]
    }
   ],
   "source": [
    "# _________ Write Output Part 2 (Run After Approval) ____________\n",
    "\n",
    "elements = set(\n",
    "    sym\n",
    "    for entry in dataset.iterate_entries()\n",
    "    for sym in entry.initial_molecule.symbols\n",
    ")\n",
    "\n",
    "print(\"\\n\\n# Output for README Part 2\\n\")\n",
    "print(\"* Description: {}\".format(dataset.description))\n",
    "print(\"* Purpose: {}\".format(dataset.tagline))\n",
    "print(\"* Name: {}\".format(dataset.name))\n",
    "print(\"* Submitter: {}\\n\".format(dataset.extras[\"submitter\"]))\n",
    "\n",
    "print(\"\\n## Metadata\")\n",
    "print(f\"* Elements: {{{', '.join(elements)}}}\")\n",
    "\n",
    "for spec, obj in dataset.specifications.items():\n",
    "    od = obj.dict()['specification']\n",
    "    print(\"\\n* Program:\", od[\"program\"])\n",
    "    od = od[\"qc_specification\"]\n",
    "    print(\"* QC Specifications:\", spec)\n",
    "    for field, value in od.items():\n",
    "        print(f\"  * {field}: {od[field]}\")\n",
    "    if \"scf_properties\" in od[\"keywords\"]:\n",
    "        print(\"  * SCF Properties:\")\n",
    "        for field in od[\"keywords\"][\"scf_properties\"]:\n",
    "            print(f\"    * {field}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95201f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold.to_json(dataset, filename=\"scaffold_opt.json\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68beabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Complete: 100%|██████████| 100/100 [00:26<00:00,  3.82it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal job final status: complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ij = dataset.create_view(description=\"Dataset without wavefunctions include in view\", provenance={}, include=['**'], exclude=[\"wavefunction\"], include_children=False)\n",
    "ij.watch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
